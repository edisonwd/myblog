# LLMç»Ÿä¸€ç½‘å…³ï¼šLiteLLM è¯¦ç»†ä»‹ç»

## LiteLLM åŸºæœ¬ä»‹ç»

#### ä¸ºä»€ä¹ˆéœ€è¦LiteLLMï¼Ÿ

åœ¨AIåº”ç”¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¼€å‘è€…å¸¸å¸¸é‡åˆ°ä»¥ä¸‹ç—›ç‚¹ï¼š
- æ¥å£ç¢ç‰‡åŒ–ï¼šä¸åŒå‚å•†çš„APIæ¥å£å·®å¼‚å·¨å¤§ï¼Œä»å‚æ•°å‘½ååˆ°å“åº”æ ¼å¼éƒ½ä¸ç»Ÿä¸€
- å¯†é’¥ç®¡ç†å¤æ‚ï¼šæ¯ä¸ªæœåŠ¡å•†éƒ½æœ‰ç‹¬ç«‹çš„è®¤è¯æœºåˆ¶ï¼Œå¯†é’¥ç®¡ç†æˆä¸ºè¿ç»´è´Ÿæ‹…
- å¤šæ¨¡å‹ç®¡ç†æˆæœ¬ï¼šæ›´æ¢æ¨¡å‹æä¾›å•†éœ€è¦é‡å†™å¤§é‡ä»£ç ï¼Œé¡¹ç›®éš¾ä»¥çµæ´»è°ƒæ•´
- å®‰å…¨ä¸åˆè§„ï¼šåŒ»ç–—æˆ–é‡‘èåœºæ™¯éœ€ç¡®ä¿æ•°æ®ä¸ç»è¿‡ç¬¬ä¸‰æ–¹æ¨¡å‹ï¼Œéœ€è¦åœ¨è°ƒç”¨ç¬¬ä¸‰æ–¹æ¨¡å‹å‰æ‹¦æˆª
- å¼€å‘è¿­ä»£ï¼šå¿«é€Ÿå¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨ä¸šåŠ¡åœºæ™¯ä¸­çš„æ•ˆæœï¼Œéœ€ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç¼–å†™è¯„æµ‹ä»£ç 
- ç³»ç»Ÿç¨³å®šæ€§ï¼šåŠ¨æ€è·¯ç”±ä¸è´Ÿè½½å‡è¡¡ï¼Œå®¹ç¾ä¸æ•…éšœè½¬ç§»ï¼Œæ™ºèƒ½é‡è¯•ä¸é™æµ

#### é¡¹ç›®æ¦‚è¿°

**LiteLLM** æ˜¯ä¸€ä¸ªå¼€æºå·¥å…·ï¼ŒLiteLLMé€šè¿‡æä¾›ç»Ÿä¸€çš„OpenAIå…¼å®¹æ¥å£ï¼Œä½œä¸º LLM API çš„é€šç”¨é€‚é…å™¨ï¼Œç®€åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆä¸ç®¡ç†ï¼Œå…è®¸å¼€å‘äººå‘˜é€šè¿‡æ ‡å‡†åŒ–æ¥å£ä¸å„ç§å¤§æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚è¯¥é¡¹ç›®æ”¯æŒç›®å‰å¸‚é¢ä¸Šå¤§å¤šæ•°LLMæä¾›å•†ï¼ŒåŒ…æ‹¬ Anthropicã€AWS Bedrockã€AWS SageMakerã€Azure OpenAIã€deepseekã€Google Vertex AIã€OpenAIç­‰ç­‰ï¼Œé™¤äº†äº‘ç«¯äº§å“ï¼Œè¿˜æ”¯æŒæœ¬åœ°åŒ–éƒ¨ç½²å·¥å…·ï¼Œå¦‚Ollamaç­‰ã€‚

é™¤äº†ç»Ÿä¸€æ¥å£ä¹‹å¤–ï¼Œè¿˜å®ç°äº†æˆæœ¬è·Ÿè¸ªã€è®¿é—®æ§åˆ¶å’Œ API è°ƒç”¨çš„å®æ—¶ç›‘æ§ç­‰åŠŸèƒ½ã€‚è¯¥é¡¹ç›®æ ¸å¿ƒç›®æ ‡æ˜¯æ˜¯ç®€åŒ–å¤š LLM åº”ç”¨çš„å¼€å‘ï¼Œæé«˜å¹³å°å›¢é˜Ÿç®¡ç†å¤šæä¾›å•†çš„æ•ˆç‡ã€‚ç›®å‰ï¼ŒLiteLLM é›†æˆäº†100å¤šä¸ªLLMçš„è®¿é—®ã€è´¹ç”¨è·Ÿè¸ªå’Œå›é€€ã€‚

ä½¿ç”¨LiteLLMï¼Œå¯ä¸ºå¼€å‘å›¢é˜ŸèŠ‚çœæ—¶é—´å’Œç²¾åŠ›ã€‚å¼€å‘è€…æ— éœ€è‡ªå®šä¹‰é›†æˆæ¯ä¸ªæ–°çš„æ¨¡å‹ API ï¼Œæˆ–ç­‰å¾…ç‰¹å®šæä¾›å•†å‘å¸ƒSDKã€‚

#### æ ¸å¿ƒåŠŸèƒ½
1. **æ ‡å‡†åŒ–APIæ¥å£**
   - **ç»Ÿä¸€è°ƒç”¨æ ¼å¼**ï¼šæ— è®ºåº•å±‚æ¨¡å‹å¦‚ä½•ï¼Œæ‰€æœ‰è¯·æ±‚å‡ä½¿ç”¨ç›¸åŒç»“æ„ï¼ˆå¦‚OpenAIæ ¼å¼ï¼‰ï¼Œé™ä½ä»£ç é€‚é…æˆæœ¬ã€‚
   - **ç¤ºä¾‹**ï¼šè°ƒç”¨GPT-4ä¸Claude 2å‡é€šè¿‡ `litellm.completion(model="æ¨¡å‹å", messages=[...])` å®ç°ã€‚

2. **å¤šæ¨¡å‹æ”¯æŒ**
   - **è¦†ç›–ä¸»æµLLM**ï¼šæ”¯æŒåŒ…æ‹¬OpenAIã€Anthropicã€Google Vertex AIã€Hugging Faceã€Cohereç­‰20+æ¨¡å‹ï¼ŒæŒç»­æ‰©å±•ä¸­ã€‚
   - **è‡ªå®šä¹‰æ¨¡å‹æ¥å…¥**ï¼šé€šè¿‡é…ç½®å¯æ¥å…¥ç§æœ‰åŒ–éƒ¨ç½²æ¨¡å‹æˆ–å°ä¼—APIã€‚

3. **æ™ºèƒ½è·¯ç”±ä¸è´Ÿè½½å‡è¡¡**
   - **è·¯ç”±ç­–ç•¥**ï¼šæ ¹æ®æ¨¡å‹ç±»å‹ã€å¯ç”¨æ€§ã€æˆæœ¬æˆ–æ—¶å»¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æœåŠ¡èŠ‚ç‚¹ã€‚
   - **æ•…éšœè½¬ç§»**ï¼šå½“æŸæ¨¡å‹æœåŠ¡ä¸å¯ç”¨æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢è‡³å¤‡ç”¨æ¨¡å‹ï¼Œç¡®ä¿ä¸šåŠ¡è¿ç»­æ€§ã€‚

4. **ç¼“å­˜ä¸æˆæœ¬ä¼˜åŒ–**
   - **è¯·æ±‚ç¼“å­˜**ï¼šå¯¹é‡å¤æŸ¥è¯¢ç»“æœç¼“å­˜ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°ã€‚
   - **ç”¨é‡ç»Ÿè®¡**ï¼šå®æ—¶ç›‘æ§å„æ¨¡å‹Tokenæ¶ˆè€—ï¼Œç”Ÿæˆæˆæœ¬æŠ¥å‘Šï¼Œæ”¯æŒé¢„ç®—å‘Šè­¦ã€‚

5. **ç›‘æ§ä¸æ—¥å¿—**
   - **æ€§èƒ½æŒ‡æ ‡**ï¼šè®°å½•å“åº”æ—¶é—´ã€é”™è¯¯ç‡ã€Tokenç”¨é‡ç­‰å…³é”®æŒ‡æ ‡ã€‚
   - **å®¡è®¡æ—¥å¿—**ï¼šè¿½è¸ªæ‰€æœ‰è¯·æ±‚è¯¦æƒ…ï¼Œä¾¿äºè°ƒè¯•ä¸åˆè§„å®¡æŸ¥ã€‚

6. **å®‰å…¨å¢å¼º**
   - **APIå¯†é’¥ç®¡ç†**ï¼šé›†ä¸­å­˜å‚¨åŠ å¯†å¯†é’¥ï¼Œé¿å…ç¡¬ç¼–ç æ³„éœ²é£é™©ã€‚
   - **è®¿é—®æ§åˆ¶**ï¼šæ”¯æŒåŸºäºè§’è‰²çš„æƒé™ç®¡ç†ï¼ˆRBACï¼‰ï¼Œé™åˆ¶æ•æ„Ÿæ“ä½œã€‚

#### æŠ€æœ¯æ¶æ„
LiteLLMé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå…³é”®ç»„ä»¶åŒ…æ‹¬ï¼š
- **APIç½‘å…³**ï¼šæ¥æ”¶è¯·æ±‚å¹¶è½¬å‘è‡³é€‚é…å™¨ï¼Œå¤„ç†è®¤è¯ã€é™æµç­‰ã€‚
- **æ¨¡å‹é€‚é…å™¨**ï¼šå°†ç»Ÿä¸€APIè½¬æ¢ä¸ºå„LLMåŸç”Ÿæ¥å£ï¼ˆå¦‚å°†OpenAIæ ¼å¼è½¬ä¸ºAnthropicçš„HTTPå‚æ•°ï¼‰ã€‚
- **è·¯ç”±å¼•æ“**ï¼šåŠ¨æ€å†³ç­–æ¨¡å‹è°ƒç”¨è·¯å¾„ï¼Œæ”¯æŒè‡ªå®šä¹‰è§„åˆ™ï¼ˆå¦‚â€œä¼˜å…ˆä½¿ç”¨æˆæœ¬ä½äº$0.01/1k Tokensçš„æ¨¡å‹â€ï¼‰ã€‚
- **ç¼“å­˜å±‚**ï¼šåŸºäºRedisæˆ–å†…å­˜å­˜å‚¨é«˜é¢‘æŸ¥è¯¢ç»“æœã€‚
- **ç›‘æ§ä»£ç†**ï¼šé›†æˆPrometheusã€Grafanaç­‰å·¥å…·ï¼Œæä¾›å¯è§†åŒ–çœ‹æ¿ã€‚

#### å…¸å‹åº”ç”¨åœºæ™¯
1. **å¤šæ¨¡å‹A/Bæµ‹è¯•**
   - åŒæ—¶å‘GPT-4å’ŒClaude 2å‘é€è¯·æ±‚ï¼Œæ¯”è¾ƒç”Ÿæˆè´¨é‡ï¼Œè¾…åŠ©æ¨¡å‹é€‰å‹ã€‚

2. **æ··åˆäº‘LLMè°ƒåº¦**
   - ç»“åˆå…¬æœ‰äº‘APIï¼ˆå¦‚OpenAIï¼‰ä¸æœ¬åœ°éƒ¨ç½²æ¨¡å‹ï¼ˆå¦‚Llama 2ï¼‰ï¼Œæ ¹æ®æ•°æ®æ•æ„Ÿæ€§è‡ªåŠ¨è·¯ç”±ã€‚

3. **æˆæœ¬æ•æ„Ÿå‹åº”ç”¨**
   - é…ç½®è·¯ç”±ç­–ç•¥ï¼Œåœ¨éå…³é”®ä»»åŠ¡ä¸­ä½¿ç”¨ä½ä»·æ¨¡å‹ï¼ˆå¦‚GPT-3.5ï¼‰ï¼Œå…³é”®ä»»åŠ¡åˆ‡æ¢è‡³é«˜æˆæœ¬æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ã€‚

4. **å®¹ç¾å¤‡ä»½**
   - å½“Qwenæ¨¡å‹è°ƒç”¨æœåŠ¡å‡ºç°æ•…éšœæ—¶ï¼Œè‡ªåŠ¨å°†è¯·æ±‚è½¬å‘è‡³deepseekæ¨¡å‹è°ƒç”¨æœåŠ¡ï¼Œé¿å…æœåŠ¡ä¸­æ–­ã€‚

#### ä¼˜åŠ¿å¯¹æ¯”
| ç‰¹æ€§               | åŸç”Ÿå¤šæ¨¡å‹å¼€å‘         | ä½¿ç”¨LiteLLM          |
|--------------------|------------------------|----------------------|
| ä»£ç å¤æ‚åº¦         | éœ€ä¸ºæ¯ä¸ªæ¨¡å‹ç¼–å†™é€‚é…å±‚ | ç»Ÿä¸€APIï¼Œä»£ç å¤ç”¨ç‡é«˜|
| ç»´æŠ¤æˆæœ¬           | éœ€è·Ÿè¸ªå„APIæ›´æ–°        | è‡ªåŠ¨å¤„ç†æ¥å£å˜æ›´     |
| æ•…éšœæ¢å¤           | æ‰‹åŠ¨å®ç°é‡è¯•é€»è¾‘       | å†…ç½®æ™ºèƒ½æ•…éšœè½¬ç§»     |
| æˆæœ¬ä¼˜åŒ–           | éœ€è‡ªè¡Œç»Ÿè®¡ä¸åˆ†æ       | æä¾›ç”¨é‡ä»ªè¡¨ç›˜       |

#### æ€»ç»“
LiteLLMé€šè¿‡æŠ½è±¡åŒ–åº•å±‚LLMçš„å¤æ‚æ€§ï¼Œæ˜¾è‘—é™ä½äº†å¤šæ¨¡å‹åº”ç”¨çš„å¼€å‘é—¨æ§›ã€‚å…¶æ ¸å¿ƒä»·å€¼åœ¨äºï¼š
- **å¼€å‘è€…å‹å¥½**ï¼šå‡å°‘70%ä»¥ä¸Šçš„æ¨¡å‹é›†æˆä»£ç é‡ã€‚
- **ä¼ä¸šçº§ç®¡æ§**ï¼šæä¾›ä»æˆæœ¬æ§åˆ¶åˆ°å®‰å…¨å®¡è®¡çš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚
- **ç”Ÿæ€å…¼å®¹æ€§**ï¼šæ— ç¼å¯¹æ¥ç°æœ‰MLOpså·¥å…·é“¾ï¼ˆå¦‚MLflowã€Weights & Biasesï¼‰ã€‚

éšç€LLMæŠ€æœ¯çš„å¿«é€Ÿæ¼”è¿›ï¼ŒLiteLLMæ­£æˆä¸ºæ„å»ºç¨³å¥ã€å¯æ‰©å±•AIåº”ç”¨çš„å…³é”®åŸºç¡€è®¾æ–½ã€‚


## LiteLLM ä¸¤ç§ä½¿ç”¨æ–¹å¼ä»‹ç»

LiteLLMæœ‰ä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼š
- **Python SDK**ï¼šåœ¨ Python ä»£ç ä¸­ä½¿ç”¨ LiteLLMï¼Œé€šå¸¸ç”±æ„å»º llm é¡¹ç›®çš„å¼€å‘äººå‘˜ä½¿ç”¨ã€‚
- **Proxy Server**ï¼šéœ€è¦ä¸­å¤®æœåŠ¡ï¼ˆLLM ç½‘å…³ï¼‰è®¿é—®å¤šä¸ª LLMï¼Œé€šå¸¸ç”± Gen AI æ”¯æŒ/ML å¹³å°å›¢é˜Ÿä½¿ç”¨ã€‚

### LiteLLM Proxy Server vs. Python SDKï¼šæ ¸å¿ƒåŒºåˆ«è¯¦è§£
#### **å®šä½ä¸æ¶æ„å·®å¼‚**
| **ç»´åº¦**         | **LiteLLM Python SDK**                     | **LiteLLM Proxy Server**                  |
|------------------|-------------------------------------------|-------------------------------------------|
| **æœ¬è´¨**         | è½»é‡çº§Pythonå®¢æˆ·ç«¯åº“                      | ç‹¬ç«‹éƒ¨ç½²çš„APIç½‘å…³æœåŠ¡                     |
| **è¿è¡Œæ–¹å¼**     | åµŒå…¥åº”ç”¨ä»£ç ä¸­ï¼ˆå¦‚Django/Flask/fastapiï¼‰          | ç‹¬ç«‹è¿›ç¨‹ï¼ˆDocker/K8séƒ¨ç½²ï¼‰                |
| **é€šä¿¡åè®®**     | å‡½æ•°è°ƒç”¨ï¼ˆPythonè¿›ç¨‹å†…ï¼‰                  | HTTP REST APIï¼ˆè·¨è¯­è¨€è°ƒç”¨ï¼‰               |
| **ä¾èµ–**         | éœ€Pythonç¯å¢ƒ                              | éœ€è¦å•ç‹¬éƒ¨ç½²ï¼ˆä»»ä½•HTTPå®¢æˆ·ç«¯å¯è®¿é—®ï¼Œæ— è¯­è¨€é™åˆ¶ï¼‰        |

### ä¸€ã€**LiteLLM Python SDK**
#### æ ¸å¿ƒç‰¹å¾

- **æœ¬åœ°é›†æˆ**ï¼šä½œä¸ºPythonåŒ…ç›´æ¥å®‰è£…ï¼ˆ`pip install litellm`ï¼‰
- **å¼€å‘è€…å‹å¥½**ï¼šé¢å‘Pythonå¼€å‘è€…ï¼Œæä¾›åŒæ­¥/å¼‚æ­¥API
- **æ— æœåŠ¡ä¾èµ–**ï¼šæ— éœ€é¢å¤–åŸºç¡€è®¾æ–½

#### å…¸å‹ä½¿ç”¨åœºæ™¯
1. **å•åº”ç”¨å¿«é€Ÿé›†æˆ**
   ```python
   import litellm  # å•åº“é›†æˆæ‰€æœ‰åŠŸèƒ½
   # ç›´æ¥è°ƒç”¨æ¨¡å‹
   response = litellm.completion(model="gpt-4", messages=[...])
   ```

2. **è„šæœ¬/å®éªŒç¯å¢ƒ**
   ```python
   # Jupyter Notebookä¸­å¿«é€Ÿæµ‹è¯•æ¨¡å‹
    import litellm
    model_list = litellm.model_list # æŸ¥çœ‹æ”¯æŒæ¨¡å‹
    print(len(model_list)) # 791
   ```

3. **ä¸å…¶ä»–Pythonåº“ååŒ**
   ```python
    # å®‰è£… langchain-litellm ä¾èµ–
    !pip install -qU langchain-litellm
    # æ— ç¼æ¥å…¥LangChain
    from langchain_litellm import ChatLiteLLM
    llm = ChatLiteLLM(model="gpt-4.1-nano", temperature=0.1)
   ```


### äºŒã€**LiteLLM Proxy Server**
#### ä½¿ç”¨dockeréƒ¨ç½²
```bash
docker run \
    -v $(pwd)/litellm_config.yaml:/app/config.yaml \
    -e AZURE_API_KEY=d6*********** \
    -e AZURE_API_BASE=https://openai-***********/ \
    -p 4000:4000 \
    ghcr.io/berriai/litellm:main-latest \
    --config /app/config.yaml --detailed_debug
```
#### æ ¸å¿ƒåŠŸèƒ½
- **ä¸­å¿ƒåŒ–ç½‘å…³**ï¼šç‹¬ç«‹æœåŠ¡ï¼Œé€šè¿‡HTTPæš´éœ²ç»Ÿä¸€API
- **ä¼ä¸šçº§åŠŸèƒ½**ï¼š
  - **å¤šç§Ÿæˆ·ç®¡ç†**ï¼ˆå›¢é˜Ÿ/é¡¹ç›®éš”ç¦»ï¼‰
  - **é›†ä¸­å¼ç›‘æ§é¢æ¿**
  - **å…¨å±€é€Ÿç‡é™åˆ¶**
  - **æˆæœ¬è·Ÿè¸ª**

#### å…¸å‹ä½¿ç”¨åœºæ™¯
1. **å¤šè¯­è¨€æ¶æ„**

   ä½¿ç”¨ javascript è°ƒç”¨
   ```javascript
   // å‰ç«¯JSç›´æ¥è°ƒç”¨
   fetch("http://llm-gateway.company.com/completions", {
     method: "POST",
     headers: { 
       "Authorization": "Bearer team-123",
       "Content-Type": "application/json"
     },
     body: JSON.stringify({
       model: "gpt-4",
       messages: [{role: "user", content: "Hello"}]
     })
   })
   ```
   ä½¿ç”¨ curl è°ƒç”¨
   ```shell
    curl -X POST 'http://0.0.0.0:4000/chat/completions' \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer sk-1234' \
    -d '{
        "model": "gpt-4o",
        "messages": [
          {
            "role": "system",
            "content": "You are an LLM named gpt-4o"
          },
          {
            "role": "user",
            "content": "what is your name?"
          }
        ]
    }'
   ```
   ä½¿ç”¨ openai å…¼å®¹æ¥å£è°ƒç”¨

   ```python
    import openai # openai v1.0.0+
    client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url
    messages = [{
                    "role": "user",
                    "content": "this is a test request, write a short poem"
                }]
    # request sent to model set on litellm proxy, `litellm --model`
    response = client.chat.completions.create(model="gpt-3.5-turbo", messages=messages )
    
    print(response)
   ```

### ä¸‰ã€æ ¸å¿ƒèƒ½åŠ›å¯¹æ¯”
| **åŠŸèƒ½**               | **Python SDK**                          | **Proxy Server**                         |
|------------------------|-----------------------------------------|------------------------------------------|
| **å¤šæ¨¡å‹è°ƒç”¨**         | âœ… æ”¯æŒ                                 | âœ… æ”¯æŒ                                  |
| **è´Ÿè½½å‡è¡¡**           | âœ… åŸºç¡€è·¯ç”±ç­–ç•¥                         | âœ… é«˜çº§ç®—æ³•ï¼ˆä¸€è‡´æ€§å“ˆå¸Œ/æƒé‡è½®è¯¢ï¼‰       |
| **æˆæœ¬è·Ÿè¸ª**           | âœ… å•è¿›ç¨‹çº§ç»Ÿè®¡                         | âœ… å¤šå›¢é˜Ÿç»†ç²’åº¦åˆ†æ                      |
| **å¯†é’¥ç®¡ç†**           | âŒ éœ€è‡ªè¡Œå®ç°åŠ å¯†                       | âœ… é›†ä¸­å­˜å‚¨ï¼ˆVaulté›†æˆï¼‰                 |
| **è®¿é—®æ§åˆ¶**           | âŒ æ—                                    | âœ… RBAC/å›¢é˜Ÿéš”ç¦»                         |
| **å…¨å±€é€Ÿç‡é™åˆ¶**       | âŒ ä»…å•è¿›ç¨‹æœ‰æ•ˆ                         | âœ… é›†ç¾¤çº§æ§åˆ¶                            |
| **å®¡è®¡æ—¥å¿—**           | âœ… åŸºç¡€æ—¥å¿—                             | âœ… ç»“æ„åŒ–æ—¥å¿—ï¼ˆElasticSearché›†æˆï¼‰       |
| **æœåŠ¡å‘ç°**           | âŒ æ—                                    | âœ… å¥åº·æ£€æŸ¥+è‡ªåŠ¨æ•…éšœè½¬ç§»                 |

### å››ã€å¦‚ä½•é€‰æ‹©ï¼Ÿ
#### é€‰æ‹© **Python SDK** ï¼š
- æ„å»ºçº¯Pythonåº”ç”¨ï¼ˆå¦‚AIåŠ©æ‰‹è„šæœ¬ï¼‰
- éœ€è¦æç®€åŸå‹éªŒè¯ï¼ˆPOCé˜¶æ®µï¼‰
- æ— å¤šå›¢é˜Ÿ/å¤šè¯­è¨€é›†æˆéœ€æ±‚

#### é€‰æ‹© **Proxy Server** ï¼š
- ä¼ä¸šç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- éœ€è¦æ”¯æŒJava/Go/Node.jsç­‰å¤šè¯­è¨€å®¢æˆ·ç«¯
- è¦æ±‚å›¢é˜Ÿéš”ç¦»ä¸SLAä¿éšœ
- å·²æœ‰Prometheus/Grafanaç›‘æ§æ ˆ

### äº”ã€ååŒä½¿ç”¨æ–¹æ¡ˆ
```mermaid
graph LR
    A[Pythonåº”ç”¨] -->|ç›´æ¥è°ƒç”¨| B(Python SDK)
    C[Javaåº”ç”¨] -->|HTTP| D(Proxy Server)
    E[Node.jsåº”ç”¨] -->|HTTP| D
   
    D -->|è·¯ç”±è¯·æ±‚| F[OpenAI]
    D -->|è·¯ç”±è¯·æ±‚| G[Anthropic]
    D -->|è·¯ç”±è¯·æ±‚| H[æœ¬åœ°æ¨¡å‹]
    B -->|ç»•è¿‡ç½‘å…³ç›´è¿| F
    B -->|ç»•è¿‡ç½‘å…³ç›´è¿| G

```
**æœ€ä½³å®è·µ**ï¼š
- å¼€å‘ç¯å¢ƒç”¨SDKå¿«é€Ÿè¿­ä»£
- ç”Ÿäº§ç¯å¢ƒé€šè¿‡Proxyç»Ÿä¸€ç®¡ç†
- SDKåº”ç”¨å¯é€æ­¥è¿ç§»è‡³Proxy

## LiteLLM å®è·µ

å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯pythonåº”ç”¨ï¼Œå¹¶ä¸”ä¸»è¦ä½¿ç”¨çš„æ˜¯ **langchain** æ¡†æ¶ï¼Œæ‰€ä»¥æ¥ä¸‹æ¥çš„å®è·µå†…å®¹ä»¥ **LiteLLM Python SDK** ä¸ºä¸»ï¼Œé‡ç‚¹ä»‹ç»ä¸‹é¢åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•ã€‚

- éæµå¼å¯¹è¯
- æµå¼å¯¹è¯
- å‡½æ•°è°ƒç”¨
- åŠ¨æ€è·¯ç”±
- æ™ºèƒ½é‡è¯•

è¿™é‡Œä»ä¸¤ä¸ªå¤§çš„æ–¹é¢ä»‹ç»è¿™äº›åŠŸèƒ½çš„ä½¿ç”¨
1. ä½¿ç”¨ **LiteLLM Python SDK** ç›´æ¥æä¾›çš„æ¥å£
2. ä¸langchainé›†æˆï¼Œä½¿ç”¨ **langchain-litellm** æä¾›çš„æ¥å£

ä¸ºäº†æ›´å¥½çš„å®è·µï¼Œæ¥ä¸‹æ¥ä» [deepseek](https://api-docs.deepseek.com/zh-cn/) å’Œ [é˜¿é‡Œäº‘ç™¾ç‚¼](https://www.aliyun.com/product/bailian) è¿™ä¸¤ä¸ªå¹³å°ç”³è¯·æ¨¡å‹è°ƒç”¨æœåŠ¡ã€‚
- ç”³è¯· deepseek æ¨¡å‹è°ƒç”¨æœåŠ¡éœ€è¦å……å€¼ï¼Œå…·ä½“ä½¿ç”¨å¯ä»¥æŸ¥çœ‹[deepseek](https://api-docs.deepseek.com/zh-cn/)å®˜æ–¹æ–‡æ¡£ã€‚
- ç”³è¯·é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹è°ƒç”¨æœåŠ¡ï¼Œé¦–æ¬¡å¼€é€šæ¯ä¸ªæ¨¡å‹å¯ä»¥å…è´¹é¢†å–100ä¸‡tokenä½¿ç”¨é¢åº¦ï¼Œå…·ä½“ä½¿ç”¨å¯ä»¥æŸ¥çœ‹[é˜¿é‡Œäº‘ç™¾ç‚¼](https://www.aliyun.com/product/bailian)å®˜æ–¹æ–‡æ¡£ã€‚

å‡è®¾ä½ å·²ç»ç”³è¯·å¥½äº†å¯¹åº”çš„ **api_key** ï¼Œæ¥ä¸‹æ¥å¼€å§‹å®è·µã€‚

### LiteLLM Python SDK çš„ åŸºæœ¬ä½¿ç”¨

#### å®‰è£…ä¾èµ–
```bash
pip install litellm -U
```

#### litellm.completionæ–¹æ³•å‚æ•°è¯¦è§£

ä½¿ç”¨ **LiteLLM Python SDK** ä¸»è¦çš„æ¥å£æ˜¯ **litellm.completion()**ï¼ŒæŸ¥çœ‹æºç å‚æ•°åŠè¯´æ˜å¦‚ä¸‹ï¼š

```python
@client
def completion(  # type: ignore # noqa: PLR0915
    model: str,
    # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create
    messages: List = [],
    timeout: Optional[Union[float, str, httpx.Timeout]] = None,
    temperature: Optional[float] = None,
    top_p: Optional[float] = None,
    n: Optional[int] = None,
    stream: Optional[bool] = None,
    stream_options: Optional[dict] = None,
    stop=None,
    max_completion_tokens: Optional[int] = None,
    max_tokens: Optional[int] = None,
    modalities: Optional[List[ChatCompletionModality]] = None,
    prediction: Optional[ChatCompletionPredictionContentParam] = None,
    audio: Optional[ChatCompletionAudioParam] = None,
    presence_penalty: Optional[float] = None,
    frequency_penalty: Optional[float] = None,
    logit_bias: Optional[dict] = None,
    user: Optional[str] = None,
    # openai v1.0+ new params
    reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
    response_format: Optional[Union[dict, Type[BaseModel]]] = None,
    seed: Optional[int] = None,
    tools: Optional[List] = None,
    tool_choice: Optional[Union[str, dict]] = None,
    logprobs: Optional[bool] = None,
    top_logprobs: Optional[int] = None,
    parallel_tool_calls: Optional[bool] = None,
    web_search_options: Optional[OpenAIWebSearchOptions] = None,
    deployment_id=None,
    extra_headers: Optional[dict] = None,
    # soon to be deprecated params by OpenAI
    functions: Optional[List] = None,
    function_call: Optional[str] = None,
    # set api_base, api_version, api_key
    base_url: Optional[str] = None,
    api_version: Optional[str] = None,
    api_key: Optional[str] = None,
    model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.
    # Optional liteLLM function params
    thinking: Optional[AnthropicThinkingParam] = None,
    **kwargs,
) -> Union[ModelResponse, CustomStreamWrapper]:
```

æ³¨æ„ï¼šç”±äºå‚æ•°å¾ˆå¤šï¼Œæˆ‘ä¼šå°½é‡ç®€æ´æ˜äº†åœ°è§£é‡Šæ¯ä¸ªå‚æ•°ã€‚

1. `model: str`ï¼šæŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "gpt-3.5-turbo"ã€‚

2. `messages: List = []`ï¼šèŠå¤©æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«"role"ï¼ˆå¦‚"user"ã€"assistant"ï¼‰å’Œ"content"ã€‚

3. `timeout: Optional[Union[float, str, httpx.Timeout]] = None`ï¼šè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œå¯ä»¥æ˜¯ç§’æ•°ï¼ˆfloatï¼‰ï¼Œå­—ç¬¦ä¸²ï¼ˆå¦‚"5s"ï¼‰æˆ–httpx.Timeoutå¯¹è±¡ã€‚

4. `temperature: Optional[float] = None`ï¼šæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚å€¼è¶Šé«˜ï¼Œè¾“å‡ºè¶Šéšæœºï¼›å€¼è¶Šä½ï¼Œè¾“å‡ºè¶Šç¡®å®šã€‚

5. `top_p: Optional[float] = None`ï¼šæ ¸é‡‡æ ·ï¼ˆnucleus samplingï¼‰çš„å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚ä¸temperatureç±»ä¼¼ï¼Œä½†é€šå¸¸åªè€ƒè™‘æ¦‚ç‡è´¨é‡ç´¯è®¡åˆ°top_pçš„è¯æ±‡ã€‚

6. `n: Optional[int] = None`ï¼šç”Ÿæˆå¤šä¸ªç‹¬ç«‹çš„å›å¤ï¼ˆé€‰æ‹©å…¶ä¸­ä¸€ä¸ªè¿”å›ï¼‰ã€‚

7. `stream: Optional[bool] = None`ï¼šæ˜¯å¦ä»¥æµå¼ï¼ˆstreamï¼‰æ–¹å¼è¿”å›å“åº”ã€‚

8. `stream_options: Optional[dict] = None`ï¼šæµå¼ä¼ è¾“çš„é¢å¤–é€‰é¡¹ã€‚

9. `stop`ï¼šå¯é€‰ï¼ŒæŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ªå­—ç¬¦ä¸²ï¼Œå½“æ¨¡å‹ç”Ÿæˆè¿™äº›å­—ç¬¦ä¸²æ—¶åœæ­¢ç”Ÿæˆã€‚

10. `max_completion_tokens: Optional[int] = None`ï¼šé™åˆ¶å®Œæˆéƒ¨åˆ†çš„æœ€å¤§tokenæ•°ï¼ˆä¸åŒ…æ‹¬æç¤ºï¼‰ã€‚

11. `max_tokens: Optional[int] = None`ï¼šæ•´ä¸ªç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼ˆåŒ…æ‹¬æç¤ºå’Œå®Œæˆéƒ¨åˆ†ï¼‰ã€‚é€šå¸¸ç”¨è¿™ä¸ªæˆ–max_completion_tokensã€‚

12. `modalities: Optional[List[ChatCompletionModality]] = None`ï¼šæŒ‡å®šäº¤äº’çš„æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒç­‰ï¼‰ï¼Œå¯èƒ½æ˜¯å¤šæ¨¡æ€è¾“å…¥ã€‚

13. `prediction: Optional[ChatCompletionPredictionContentParam] = None`ï¼šé¢„æµ‹å†…å®¹å‚æ•°ï¼Œå¯èƒ½ç”¨äºæŒ‡å®šç”Ÿæˆå†…å®¹çš„ç»“æ„æˆ–ç±»å‹ã€‚

14. `audio: Optional[ChatCompletionAudioParam] = None`ï¼šéŸ³é¢‘å‚æ•°ï¼Œå¯èƒ½ç”¨äºè¯­éŸ³è¾“å…¥æˆ–è¾“å‡ºã€‚

15. `presence_penalty: Optional[float] = None`ï¼šå­˜åœ¨æƒ©ç½šï¼Œç”¨äºé™ä½å·²ç»å‡ºç°è¿‡çš„tokençš„æ¦‚ç‡ã€‚

16. `frequency_penalty: Optional[float] = None`ï¼šé¢‘ç‡æƒ©ç½šï¼Œç”¨äºé™ä½é¢‘ç¹å‡ºç°çš„tokençš„æ¦‚ç‡ã€‚

17. `logit_bias: Optional[dict] = None`ï¼šä¸€ä¸ªå­—å…¸ï¼Œç”¨äºè°ƒæ•´ç‰¹å®štokençš„ç”Ÿæˆæ¦‚ç‡ï¼ˆé€šè¿‡è°ƒæ•´logitå€¼ï¼‰ã€‚

18. `user: Optional[str] = None`ï¼šç»ˆç«¯ç”¨æˆ·çš„æ ‡è¯†ç¬¦ï¼Œç”¨äºç›‘æ§å’Œæ»¥ç”¨æ£€æµ‹ã€‚

19. `reasoning_effort: Optional[Literal["low", "medium", "high"]] = None`ï¼šæŒ‡å®šæ¨¡å‹æ¨ç†çš„åŠªåŠ›ç¨‹åº¦ï¼ˆå¯èƒ½å½±å“å“åº”æ—¶é—´å’Œè´¨é‡ï¼‰ã€‚

20. `response_format: Optional[Union[dict, Type[BaseModel]]] = None`ï¼šæŒ‡å®šå“åº”æ ¼å¼ï¼Œæ¯”å¦‚JSONå¯¹è±¡ï¼Œæˆ–è€…ä½¿ç”¨Pydanticçš„BaseModelå®šä¹‰ç»“æ„ã€‚

21. `seed: Optional[int] = None`ï¼šéšæœºç§å­ï¼Œç”¨äºä½¿ç”Ÿæˆç»“æœå…·æœ‰ç¡®å®šæ€§ã€‚

22. `tools: Optional[List] = None`ï¼šä¸€ä¸ªå·¥å…·åˆ—è¡¨ï¼Œæ¨¡å‹å¯ä»¥è°ƒç”¨è¿™äº›å·¥å…·ï¼ˆç±»ä¼¼äºfunctionsçš„æ›¿ä»£ï¼‰ã€‚

23. `tool_choice: Optional[Union[str, dict]] = None`ï¼šæ§åˆ¶æ¨¡å‹å¦‚ä½•é€‰æ‹©å·¥å…·ï¼Œå¯ä»¥æ˜¯"auto"ã€"none"ï¼Œæˆ–æŒ‡å®šæŸä¸ªå·¥å…·ã€‚

24. `logprobs: Optional[bool] = None`ï¼šæ˜¯å¦è¿”å›æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ã€‚

25. `top_logprobs: Optional[int] = None`ï¼šå¦‚æœlogprobsä¸ºTrueï¼Œè¿™ä¸ªå‚æ•°æŒ‡å®šè¿”å›æ¯ä¸ªä½ç½®æ¦‚ç‡æœ€é«˜çš„å‡ ä¸ªtokenåŠå…¶å¯¹æ•°æ¦‚ç‡ã€‚

26. `parallel_tool_calls: Optional[bool] = None`ï¼šæ˜¯å¦å…è®¸æ¨¡å‹å¹¶è¡Œè°ƒç”¨å¤šä¸ªå·¥å…·ï¼ˆå¦‚æœæ”¯æŒï¼‰ã€‚

27. `web_search_options: Optional[OpenAIWebSearchOptions] = None`ï¼šç½‘ç»œæœç´¢é€‰é¡¹ï¼Œå¯èƒ½ç”¨äºè®©æ¨¡å‹åœ¨ç”Ÿæˆå‰è¿›è¡Œç½‘ç»œæœç´¢ã€‚

28. `deployment_id=None`ï¼šéƒ¨ç½²IDï¼Œå¯èƒ½ç”¨äºAzure OpenAIæœåŠ¡ï¼ŒæŒ‡å®šç‰¹å®šçš„éƒ¨ç½²ã€‚

29. `extra_headers: Optional[dict] = None`ï¼šé¢å¤–çš„HTTPå¤´ï¼Œç”¨äºè‡ªå®šä¹‰è¯·æ±‚ã€‚

30. `functions: Optional[List] = None`ï¼šå‡½æ•°åˆ—è¡¨ï¼ˆæ—§ç‰ˆï¼‰ï¼Œæ¨¡å‹å¯ä»¥è°ƒç”¨è¿™äº›å‡½æ•°ï¼ˆå°†è¢«toolså–ä»£ï¼‰ã€‚

31. `function_call: Optional[str] = None`ï¼šæ§åˆ¶å‡½æ•°è°ƒç”¨è¡Œä¸ºï¼Œå¦‚"auto"ã€"none"æˆ–å¼ºåˆ¶è°ƒç”¨ç‰¹å®šå‡½æ•°ã€‚

32. `base_url: Optional[str] = None`ï¼šAPIçš„åŸºç¡€URLï¼Œç”¨äºè¦†ç›–é»˜è®¤çš„OpenAI APIåœ°å€ã€‚

33. `api_version: Optional[str] = None`ï¼šAPIç‰ˆæœ¬ï¼Œç‰¹åˆ«ç”¨äºAzure OpenAIæœåŠ¡ã€‚

34. `api_key: Optional[str] = None`ï¼šAPIå¯†é’¥ï¼Œç”¨äºè®¤è¯ã€‚

35. `model_list: Optional[list] = None`ï¼šä¸€ä¸ªåˆ—è¡¨ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªAPIçš„é…ç½®ï¼ˆå¦‚å¤šä¸ªbase_urlå’Œapi_keyï¼‰ï¼Œç”¨äºè´Ÿè½½å‡è¡¡æˆ–å›é€€ã€‚

36. `thinking: Optional[AnthropicThinkingParam] = None`ï¼šç‰¹å®šäºAnthropicæ¨¡å‹çš„å‚æ•°ï¼Œå¯èƒ½ç”¨äºæ§åˆ¶æ¨¡å‹çš„â€œæ€è€ƒâ€è¿‡ç¨‹ã€‚

37. `**kwargs`ï¼šå…¶ä»–æœªæŒ‡å®šçš„å‚æ•°ï¼Œç”¨äºå‘å‰æˆ–å‘åå…¼å®¹ã€‚

è¿”å›ç±»å‹ï¼š`Union[ModelResponse, CustomStreamWrapper]`ï¼Œè¡¨ç¤ºè¿”å›ä¸€ä¸ªæ¨¡å‹å“åº”å¯¹è±¡ï¼Œæˆ–è€…å¦‚æœæ˜¯æµå¼å“åº”ï¼Œåˆ™è¿”å›ä¸€ä¸ªè‡ªå®šä¹‰çš„æµåŒ…è£…å™¨ã€‚

> æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°æ˜¯ä¸€ä¸ªå°è£…å‡½æ•°ï¼Œç”¨äºç»Ÿä¸€è°ƒç”¨ä¸åŒæä¾›è€…ï¼ˆå¦‚OpenAIã€Anthropicç­‰ï¼‰çš„APIï¼Œå› æ­¤å‚æ•°éå¸¸ä¸°å¯Œï¼Œæ¶µç›–äº†å¤šä¸ªæä¾›è€…çš„ç‰¹å®šå‚æ•°ã€‚

ä¸ºäº†ä¾¿äºç†è§£å„ä¸ªå‚æ•°çš„ä½œç”¨ï¼Œæ¥ä¸‹æ¥å°†æŒ‰ç…§å‚æ•°çš„åŠŸèƒ½åˆ†ç±»ï¼Œæ•´ç†æˆè¡¨æ ¼ï¼Œä»¥ä¸‹æ˜¯ `completion` å‡½æ•°å‚æ•°çš„åˆ†ç±»æ•´ç†è¡¨æ ¼ï¼š

##### **åŸºç¡€å‚æ•°**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `model` | `str` | - | **å¿…éœ€**ï¼Œæ¨¡å‹åç§°ï¼ˆå¦‚ `gpt-4-turbo`ï¼‰ |
| `messages` | `List` | `[]` | **å¿…éœ€**ï¼ŒèŠå¤©æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ ¼å¼ï¼š`[{"role": "user", "content": "text"}]`ï¼‰ |


##### **ç”Ÿæˆæ§åˆ¶å‚æ•°**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `temperature` | `float` | `None` | éšæœºæ€§æ§åˆ¶ï¼ˆ0.0-2.0ï¼‰ï¼Œå€¼è¶Šé«˜è¾“å‡ºè¶Šéšæœº |
| `top_p` | `float` | `None` | æ ¸é‡‡æ ·é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰ï¼Œä»…ç´¯ç§¯æ¦‚ç‡è¶…è¿‡é˜ˆå€¼çš„è¯å‚ä¸é‡‡æ · |
| `max_tokens` | `int` | `None` | **è¾“å…¥+è¾“å‡º**æ€» token æ•°ä¸Šé™ |
| `max_completion_tokens` | `int` | `None` | **ä»…è¾“å‡º** token æ•°ä¸Šé™ |
| `stop` | `Union[str, List[str]]` | `None` | åœæ­¢ç”Ÿæˆæ ‡è®°ï¼ˆå¦‚ `["\n", "."]`ï¼‰ |
| `n` | `int` | `None` | ç”Ÿæˆå¤šæ¡ç‹¬ç«‹å›å¤çš„æ•°é‡ |
| `seed` | `int` | `None` | éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç° |


##### **é«˜çº§æ§åˆ¶å‚æ•°**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `presence_penalty` | `float` | `None` | æƒ©ç½šå·²å‡ºç° tokenï¼ˆ-2.0~2.0ï¼‰ï¼Œé¿å…é‡å¤å†…å®¹ |
| `frequency_penalty` | `float` | `None` | æƒ©ç½šé«˜é¢‘ tokenï¼ˆ-2.0~2.0ï¼‰ï¼Œé¿å…å¸¸è§è¯æ»¥ç”¨ |
| `logit_bias` | `dict` | `None` | ç‰¹å®š token æ¦‚ç‡åç§»ï¼ˆå¦‚ `{12345: 5.0}` æé«˜è¯¥ token æ¦‚ç‡ï¼‰ |
| `logprobs` | `bool` | `None` | æ˜¯å¦è¿”å› token å¯¹æ•°æ¦‚ç‡ |
| `top_logprobs` | `int` | `None` | è¿”å›æ¯ä¸ªä½ç½®æ¦‚ç‡æœ€é«˜çš„ token æ•°é‡ï¼ˆéœ€ `logprobs=True`ï¼‰ |
| `user` | `str` | `None` | ç»ˆç«¯ç”¨æˆ· IDï¼ˆç”¨äºæ»¥ç”¨ç›‘æ§ï¼‰ |


##### **å¤šæ¨¡æ€ä¸å·¥å…·è°ƒç”¨**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `modalities` | `List[ChatCompletionModality]` | `None` | å¤šæ¨¡æ€è¾“å…¥ç±»å‹ï¼ˆæ–‡æœ¬/å›¾åƒ/éŸ³é¢‘ï¼‰ |
| `audio` | `ChatCompletionAudioParam` | `None` | éŸ³é¢‘è¾“å…¥å‚æ•°ï¼ˆå¦‚è¯­éŸ³è½¬æ–‡æœ¬ï¼‰ |
| `tools` | `List` | `None` | **æ–°ç‰ˆ**å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆæ›¿ä»£ `functions`ï¼‰ |
| `tool_choice` | `Union[str, dict]` | `None` | å·¥å…·è°ƒç”¨æ§åˆ¶ï¼ˆ`"auto"`/`"none"` æˆ–æŒ‡å®šå·¥å…·ï¼‰ |
| `parallel_tool_calls` | `bool` | `None` | æ˜¯å¦å…è®¸å¹¶è¡Œè°ƒç”¨å¤šä¸ªå·¥å…· |
| `functions` | `List` | `None` | **æ—§ç‰ˆ**å‡½æ•°åˆ—è¡¨ï¼ˆå³å°†å¼ƒç”¨ï¼‰ |
| `function_call` | `str` | `None` | **æ—§ç‰ˆ**å‡½æ•°è°ƒç”¨æ§åˆ¶ï¼ˆå³å°†å¼ƒç”¨ï¼‰ |


##### **æµå¼ä¼ è¾“**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `stream` | `bool` | `None` | æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“ |
| `stream_options` | `dict` | `None` | æµå¼é…ç½®ï¼ˆå¦‚ `{"include_usage": true}`ï¼‰ |


##### **æ¨¡å‹æ¨ç†æ§åˆ¶**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `reasoning_effort` | `Literal["low","medium","high"]` | `None` | æ¨ç†å¼ºåº¦ï¼ˆå½±å“å“åº”è´¨é‡ä¸å»¶è¿Ÿï¼‰ |
| `prediction` | `ChatCompletionPredictionContentParam` | `None` | é¢„æµ‹å†…å®¹ç»“æ„åŒ–å‚æ•° |
| `thinking` | `AnthropicThinkingParam` | `None` | Anthropic æ¨¡å‹ä¸“ç”¨æ€è€ƒæ§åˆ¶ |


##### **æœç´¢ä¸å“åº”æ ¼å¼**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `web_search_options` | `OpenAIWebSearchOptions` | `None` | ç½‘ç»œæœç´¢é…ç½®ï¼ˆå¦‚å¯ç”¨å®æ—¶æœç´¢ï¼‰ |
| `response_format` | `Union[dict, Type[BaseModel]]` | `None` | å“åº”æ ¼å¼ï¼ˆå¦‚ `{"type": "json_object"}` æˆ– Pydantic æ¨¡å‹ï¼‰ |


##### **API é…ç½®**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `api_key` | `str` | `None` | è¦†ç›–é»˜è®¤ API å¯†é’¥ |
| `base_url` | `str` | `None` | è¦†ç›– API åœ°å€ï¼ˆå¦‚ Azure ç«¯ç‚¹ï¼‰ |
| `api_version` | `str` | `None` | æŒ‡å®š API ç‰ˆæœ¬ï¼ˆAzure å¿…éœ€ï¼‰ |
| `model_list` | `list` | `None` | å¤šæ¨¡å‹é…ç½®åˆ—è¡¨ï¼ˆè´Ÿè½½å‡è¡¡/æ•…éšœè½¬ç§»ï¼‰ |
| `deployment_id` | `str` | `None` | Azure éƒ¨ç½² ID |


##### **å…¶ä»–å‚æ•°**
| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `timeout` | `Union[float, str, httpx.Timeout]` | `None` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’æˆ– `httpx.Timeout` å¯¹è±¡ï¼‰ |
| `extra_headers` | `dict` | `None` | è‡ªå®šä¹‰ HTTP å¤´ |
| `kwargs` | `Any` | - | å…¼å®¹æœªæ¥æ‰©å±•çš„é¢å¤–å‚æ•° |


##### **è¿”å›å€¼**
| ç±»å‹ | è¯´æ˜ |
|------|------|
| `ModelResponse` | éæµå¼å“åº”æ—¶çš„ç»“æ„åŒ–å¯¹è±¡ |
| `CustomStreamWrapper` | æµå¼å“åº”æ—¶çš„è¿­ä»£å™¨åŒ…è£…å™¨ |

> **å…³é”®è¯´æ˜**ï¼š
> 1. éƒ¨åˆ†å‚æ•°ï¼ˆå¦‚ `modalities`ã€`audio`ï¼‰éœ€ç‰¹å®šæ¨¡å‹æ”¯æŒï¼ˆå¦‚ GPT-4oï¼‰
> 2. `tools` å·²å–ä»£ `functions`ï¼Œæ¨èä½¿ç”¨æ–°å‚æ•°
> 3. openaiæ¥å£å®Œæ•´æ–‡æ¡£å‚è€ƒï¼š[OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference/chat/create)(https://platform.openai.com/docs/api-reference/chat/create)


### LiteLLM Python SDK å®è·µ
ä¸‹é¢çš„å®è·µä¸­ä¸»è¦ä½¿ç”¨äº† **completion** æ–¹æ³•çš„**åŸºç¡€å‚æ•°(modelå’Œmessages)å’ŒAPI é…ç½®å‚æ•°(base_url å’Œ api_key)**ã€‚

> âš ï¸æ³¨æ„ï¼šä½¿ç”¨ model å‚æ•°æ—¶éœ€è¦åœ¨æ¨¡å‹åç§°å‰é¢æ·»åŠ  provider çš„åç§°ï¼ŒæŸ¥çœ‹ [litellm æ”¯æŒçš„ provider](https://docs.litellm.ai/docs/providers) ã€‚
> åœ¨ LiteLLM ä¸­ï¼Œ`model` å‚æ•°çš„è®¾è®¡éå¸¸çµæ´»ï¼Œå®ƒå…è®¸ä½ é€šè¿‡ä¸€ä¸ªå­—ç¬¦ä¸²åŒæ—¶æŒ‡å®šæ¨¡å‹åç§°å’Œæä¾›å•†ï¼ˆproviderï¼‰ï¼Œè¿™ç§è®¾è®¡ä¸»è¦æ˜¯ä¸ºäº†ç®€åŒ–å¤šæ¨¡å‹ã€å¤šæä¾›å•†çš„è°ƒç”¨ç®¡ç†ã€‚
>
> **provider åç§°çš„ä½œç”¨**
> 1. ç¡®å®š API ç«¯ç‚¹ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®š base_url å‚æ•°ï¼Œé‚£ä¹ˆåˆ™LiteLLM æ ¹æ® provider åç§°ç¡®å®šåº”è¯¥å°†è¯·æ±‚å‘é€åˆ°å“ªä¸ª API ç«¯ç‚¹ã€‚
> 2. é€‰æ‹©æ­£ç¡®çš„è®¤è¯æ–¹å¼ï¼šä¸åŒçš„æä¾›å•†ä½¿ç”¨ä¸åŒçš„è®¤è¯æ–¹å¼ï¼ˆä¾‹å¦‚ API Key çš„ä½ç½®å’Œæ ¼å¼ï¼‰ã€‚LiteLLM ä¼šæ ¹æ® provider ä½¿ç”¨æ­£ç¡®çš„è®¤è¯æ–¹å¼ã€‚
> 3. è½¬æ¢è¯·æ±‚æ ¼å¼ï¼šæ¯ä¸ªæä¾›å•†çš„ API è¯·æ±‚æ ¼å¼å¯èƒ½ä¸åŒã€‚LiteLLM ä¼šå°†æ ‡å‡†çš„ `messages` åˆ—è¡¨è½¬æ¢ä¸ºç›®æ ‡æä¾›å•†æ‰€éœ€çš„æ ¼å¼ã€‚
> 4. è½¬æ¢å“åº”æ ¼å¼ï¼šLiteLLM ä¼šå°†ä¸åŒæä¾›å•†çš„å“åº”è½¬æ¢ä¸ºç»Ÿä¸€çš„æ ‡å‡†æ ¼å¼ï¼ˆç±»ä¼¼äº OpenAI çš„æ ¼å¼ï¼‰ã€‚
>
> å¦‚æœä½ æœ‰ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹æˆ–è€…æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼Œå¯èƒ½å°±éœ€è¦æ˜ç¡®æŒ‡å®š providerã€‚
> 
å› ä¸ºè¿™é‡Œå®è·µä½¿ç”¨çš„ [deepseek](https://api-docs.deepseek.com/zh-cn/) å’Œ [é˜¿é‡Œäº‘ç™¾ç‚¼](https://www.aliyun.com/product/bailian) è¿™ä¸¤ä¸ªå¹³å°æä¾›çš„æœåŠ¡æ¥å£éƒ½å…¼å®¹ **openai** æ¥å£æ ¼å¼ï¼Œæ‰€ä»¥åœ¨è®¾ç½® **model** å‚æ•°æ—¶éƒ½åº”è¯¥åœ¨æ¨¡å‹çš„åç§°å‰é¢åŠ ä¸Š **openai/**ï¼Œä¾‹å¦‚ï¼š`openai/qwen-turbo` æˆ– `openai/deepseek-chat`
#### 1.éæµå¼å¯¹è¯

##### ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°éæµå¼å¯¹è¯

å‚è€ƒæ–‡æ¡£ï¼š[é€šä¹‰åƒé—® API çš„è¾“å…¥è¾“å‡ºå‚æ•°](https://bailian.console.aliyun.com/?tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2712576.html&renderType=iframe)




```python
import os
import litellm

BAILIAN_API_KEY = "sk-a47e70844cc843cea2ba5f23663f13d5"
os.environ["BAILIAN_API_KEY"] = BAILIAN_API_KEY

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

response = litellm.completion(
    model="openai/qwen-turbo",
    messages=messages,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("BAILIAN_API_KEY"),
)

response.choices[0].message

```




    Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œè¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None})



###### ä½¿ç”¨ deepseek éæµå¼å¯¹è¯
å‚è€ƒ [deepseek å®˜æ–¹æ–‡æ¡£](https://api-docs.deepseek.com/zh-cn/)ï¼Œå¯ä»¥çŸ¥é“ `base_url=https://api.deepseek.com/`, å› ä¸º deepseek å¹³å°æä¾›çš„æ¥å£ä¹Ÿæ˜¯å…¼å®¹ openai æ¥å£çš„ï¼Œæ‰€ä»¥ model å‚æ•°éœ€è¦è®¾ç½®ä¸º `openai/deepseek-chat`ï¼Œdeepseek å…¶ä»–å¯é€‰çš„æ¨¡å‹åç§°å¯ä»¥åœ¨deepseekå®˜ç½‘æŸ¥çœ‹ã€‚

ä¸‹é¢æ˜¯ä½¿ç”¨ litellm é›†æˆ deepseek æ¨¡å‹æœåŠ¡éæµå¼æ¥å£çš„ç¤ºä¾‹ï¼š


```python
import os
import litellm

DEEPSEEK_API_KEY = "sk-783df6a1c50445d7b0994a7a5b3ef6e3"
os.environ["DEEPSEEK_API_KEY"] = DEEPSEEK_API_KEY

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

response = litellm.completion(
    model="openai/deepseek-chat",
    messages=messages,
    base_url="https://api.deepseek.com/",
    api_key=os.getenv("DEEPSEEK_API_KEY"),
)

response.choices[0].message
```




    Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘çš„ä½¿å‘½æ˜¯ä¸ºä½ æä¾›å„ç§é—®é¢˜çš„è§£ç­”ã€åˆ›æ„çµæ„Ÿã€å­¦ä¹ è¾…åŠ©ï¼Œç”šè‡³é™ªä½ èŠå¤©è§£é—·ã€‚æ— è®ºæ˜¯çŸ¥è¯†æŸ¥è¯¢ã€å†™ä½œå»ºè®®ï¼Œè¿˜æ˜¯æ—¥å¸¸å°å›°æƒ‘ï¼Œéƒ½å¯ä»¥é—®æˆ‘å“¦ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None})



#### 2.æµå¼å¯¹è¯

ä½¿ç”¨ litellm å¼€å¯æµå¼å¯¹è¯éå¸¸ç®€å•ï¼Œåªéœ€è¦åœ¨ `completion` æ–¹æ³•ä¸­æ·»åŠ å‚æ•° `stream=True` å³å¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š


```python
import os
import litellm


messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

response = litellm.completion(
    model="openai/qwen-turbo",
    messages=messages,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("BAILIAN_API_KEY"),
    stream=True,  # å¼€å¯æµå¼å¯¹è¯
)
# æµå¼å¯¹è¯completionæ–¹æ³•è¿”å›çš„æ˜¯ CustomStreamWrapperï¼Œå¯ä»¥è¿­ä»£è·å–æµå¼å¯¹è¯çš„chunk
for chunk in response:
    print(chunk.choices[0].delta or "")
```

    Delta(provider_specific_fields=None, refusal=None, content='æˆ‘æ˜¯', role='assistant', function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='é€š', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ä¹‰', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='åƒé—®ï¼Œé˜¿é‡Œå·´å·´', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='é›†å›¢æ——ä¸‹çš„é€šä¹‰', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å›ç­”é—®é¢˜ã€åˆ›ä½œ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ–‡å­—ï¼Œå¦‚å†™', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ•…äº‹ã€å…¬æ–‡', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ã€æŠ€æœ¯æ–‡æ¡£ç­‰', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ¨ç†ã€ç¼–ç¨‹ï¼Œ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ç­‰ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='é—®é¢˜æˆ–éœ€è¦å¸®åŠ©', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None)



```python
import os
import litellm

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

response = litellm.completion(
    model="openai/deepseek-chat",
    messages=messages,
    base_url="https://api.deepseek.com/",
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    stream=True,  # å¼€å¯æµå¼å¯¹è¯
)

for chunk in response:
    print(chunk.choices[0].delta or "")
```

    Delta(provider_specific_fields=None, refusal=None, content='æˆ‘æ˜¯', role='assistant', function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='Deep', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='Se', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ek', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content=' Chat', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼Œ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ç”±', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ·±åº¦', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ±‚', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ç´¢', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å…¬å¸', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='åˆ›é€ çš„', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ™ºèƒ½', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='AI', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='åŠ©æ‰‹', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ğŸ¤–', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='âœ¨', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æˆ‘çš„', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ä½¿å‘½', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ˜¯', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å¸®åŠ©ä½ ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='è§£ç­”', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å„ç§', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='é—®é¢˜', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼Œ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ— è®ºæ˜¯', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å­¦ä¹ ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ã€', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å·¥ä½œ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼Œ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='è¿˜æ˜¯', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ—¥å¸¸', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ç”Ÿæ´»ä¸­çš„', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å°', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å›°æƒ‘', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ã€‚', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æˆ‘å¯ä»¥', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='é™ªä½ ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='èŠå¤©', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ã€', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æä¾›', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å»ºè®®', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ã€', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æŸ¥æ‰¾', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ä¿¡æ¯', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼Œ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ç”šè‡³', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å¸®ä½ ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='åˆ†æ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æ–‡æ¡£', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ğŸ“š', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ğŸ’¡', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='  \n\n', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æœ‰ä»€ä¹ˆ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='æˆ‘å¯ä»¥', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å¸®', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ä½ çš„', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='å—', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ï¼Ÿ', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, refusal=None, content='ğŸ˜Š', role=None, function_call=None, tool_calls=None, audio=None)
    Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None)


##### æµå¼å¯¹è¯è¾…åŠ©å‡½æ•°
LiteLLMè¿˜æä¾›äº†ä¸€ä¸ªè¾…åŠ©å‡½æ•° `litellm.stream_chunk_builder`ï¼Œç”¨äºä»chunkåˆ—è¡¨ä¸­é‡å»ºå®Œæ•´çš„æµå“åº”ã€‚


```python
import os
import litellm

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

response = litellm.completion(
    model="openai/deepseek-chat",
    messages=messages,
    base_url="https://api.deepseek.com/",
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    stream=True,  # å¼€å¯æµå¼å¯¹è¯
)
chunks = []
for chunk in response:
    chunks.append(chunk)

model_response = litellm.stream_chunk_builder(chunks)
print(model_response)
print(model_response.choices[0].message.content)
```

    ModelResponse(id='chatcmpl-fc4abbab-0992-432a-8141-6b23098508ae', created=1749107445, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œè¿˜èƒ½å¤„ç†å„ç§æ–‡æœ¬å’Œæ–‡ä»¶å†…å®¹ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç–‘é—®ï¼Œéƒ½å¯ä»¥æ¥é—®æˆ‘ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=61, prompt_tokens=4, total_tokens=65, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))
    æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œè¿˜èƒ½å¤„ç†å„ç§æ–‡æœ¬å’Œæ–‡ä»¶å†…å®¹ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç–‘é—®ï¼Œéƒ½å¯ä»¥æ¥é—®æˆ‘ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š


##### å¼‚æ­¥æµå¼å¯¹è¯
`litellm.completion` æ–¹æ³•åœ¨è¿”å›çš„æµå¯¹è±¡ä¸­å®ç°äº†ä¸€ä¸ª`__anext__ï¼ˆï¼‰`å‡½æ•°ï¼Œè¿™å…è®¸å¯¹æµå¯¹è±¡è¿›è¡Œå¼‚æ­¥è¿­ä»£ã€‚

ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š


```python
import asyncio, os
from typing import Union, List, Dict, Optional
import logging

logger = logging.getLogger(__name__)

model_config_dict = {
    "deepseek-chat": {
        "model": "openai/deepseek-chat",
        "base_url": "https://api.deepseek.com/v1",
        "api_key": os.getenv("DEEPSEEK_API_KEY"),
    },
}

async def llm_chat(model: str, messages: Union[str, List[Dict[str, str]]], tools: Optional[List] = None, stream=True):
    logger.info(f'ä½¿ç”¨çš„æ¨¡å‹å¦‚ä¸‹ï¼š{model}')
    model_config = model_config_dict.get(model)
    if isinstance(messages, str):
        messages = [{"role": "user", "content": messages}]

    response = await litellm.acompletion(
        model=model_config.get('model'),
        base_url=model_config.get('base_url'),
        api_key=model_config.get('api_key'),
        messages=messages,
        tools=tools,
        stream=stream,
    )
    
    return response

response = asyncio.run(llm_chat(model='deepseek-chat', messages='ä½ æ˜¯è°', stream=True))
for chunk in response:
    print(chunk.choices[0].delta or "")

```

#### å‡½æ•°è°ƒç”¨
æ¥ä¸‹æ¥é‡ç‚¹ä»‹ç»LiteLLMçš„å‡½æ•°è°ƒç”¨åŠŸèƒ½ï¼ŒåŒ…æ‹¬å¦‚ä½•ä½¿ç”¨ã€æ”¯æŒçš„æ¨¡å‹ä»¥åŠåº•å±‚æœºåˆ¶

å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰å…è®¸å¤§æ¨¡å‹æ ¹æ®ç”¨æˆ·è¾“å…¥æ™ºèƒ½è°ƒç”¨é¢„å®šä¹‰çš„å·¥å…·å‡½æ•°ã€‚LiteLLMé€šè¿‡æ ‡å‡†åŒ–æ¥å£æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†çš„å‡½æ•°è°ƒç”¨åŠŸèƒ½ã€‚

å…³é”®ç‚¹ï¼š

1. ç»Ÿä¸€æ¥å£ï¼šä½¿ç”¨OpenAIæ ¼å¼çš„å‡½æ•°å®šä¹‰

2. å¤šæ¨¡å‹æ”¯æŒï¼šåŒ…æ‹¬OpenAI, Anthropic, Azure, Googleç­‰

3. è‡ªåŠ¨è½¬æ¢ï¼šå°†éOpenAIæ¨¡å‹çš„å‡½æ•°è°ƒç”¨å“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼

ä½¿ç”¨æ­¥éª¤ï¼š

1. å®šä¹‰å·¥å…·å‡½æ•°åˆ—è¡¨ï¼ˆtoolsï¼‰

2. åœ¨completionè°ƒç”¨ä¸­ä¼ å…¥toolså‚æ•°

3. è§£ææ¨¡å‹è¿”å›çš„å·¥å…·è°ƒç”¨è¯·æ±‚

4. æ‰§è¡Œæœ¬åœ°å‡½æ•°

5. å°†å‡½æ•°æ‰§è¡Œç»“æœä¼ å›æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼‰

ä¸‹é¢è¯¦ç»†å±•å¼€ï¼š

> å¦‚æœè¦ä½¿ç”¨å‡½æ•°è°ƒç”¨åŠŸèƒ½ï¼Œé‚£ä¹ˆé€‰æ‹©çš„æ¨¡å‹éœ€è¦æ”¯æŒå‡½æ•°è°ƒç”¨ï¼Œåœ¨ litellm ä¸­æä¾›äº†ä¸€ä¸ªå·¥å…·æ–¹æ³• `litellm.supports_function_calling` ç”¨æ¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå‡½æ•°è°ƒç”¨ã€‚
>
>`litellm.supports_function_calling` æ–¹æ³•æ˜¯ä» https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json ä¸­æŸ¥è¯¢æ¨¡å‹æ˜¯å¦æ”¯æŒå‡½æ•°è°ƒç”¨ã€‚
>
>å¦‚æœé€‰æ‹©çš„æ¨¡å‹æ²¡æœ‰åœ¨è¿™ä¸ªå­—å…¸ä¸­ï¼Œåˆ™æ— æ³•é€šè¿‡è¯¥æ–¹æ³•åˆ¤æ–­ï¼Œæ‰€ä»¥åº”è¯¥ä»å¹³å°ä¸Šé¢é€‰æ‹©æ”¯æŒå‡½æ•°è°ƒç”¨çš„æ¨¡å‹ã€‚
>
>æŸ¥çœ‹ deepseek å®˜ç½‘ï¼Œå¯ä»¥çœ‹åˆ° **deepseek-chat** æ”¯æŒå‡½æ•°è°ƒç”¨ã€‚


å‡½æ•°è°ƒç”¨æµç¨‹å¦‚ä¸‹ï¼š
```mermaid
graph LR
    A[ç”¨æˆ·è¯·æ±‚] --> B(æ¨¡å‹åˆ†æ)
    B --> C{æ˜¯å¦éœ€è¦å·¥å…·}
    C -- æ˜¯ --> D[è¿”å›å·¥å…·è°ƒç”¨è¯·æ±‚]
    C -- å¦ --> E[ç›´æ¥ç”Ÿæˆå›ç­”]
    D --> F[å¼€å‘è€…æ‰§è¡Œå‡½æ•°]
    F --> G[è¿”å›å‡½æ•°ç»“æœ]
    G --> H[æ¨¡å‹æ•´åˆç»“æœ]
    H --> I[æœ€ç»ˆå“åº”]
```
ä¸‹é¢æ˜¯å®è·µä»£ç ï¼š



###### 1. å®šä¹‰å·¥å…·å‡½æ•°


```python
from pydantic import BaseModel, Field

# å®šä¹‰å·¥å…·å‚æ•°ç»“æ„
class WeatherParams(BaseModel):
    location: str = Field(description="åŸå¸‚åç§°ï¼Œå¦‚ 'åŒ—äº¬'")
    unit: str = Field(description="æ¸©åº¦å•ä½", default="celsius")

# å®šä¹‰è‚¡ç¥¨æŸ¥è¯¢å·¥å…·
class StockParams(BaseModel):
    symbol: str = Field(description="è‚¡ç¥¨ä»£ç ï¼Œå¦‚ 'AAPL'")
    timeframe: str = Field(description="æ—¶é—´èŒƒå›´", default="1d")


def get_weather(params: dict):
    weather = WeatherParams(**params)
    if 'ä¸Šæµ·' in weather.location.lower():
        return {"location": "ä¸Šæµ·", "temperature": "10", "unit": "celsius"}
    elif "åŒ—äº¬" in weather.location.lower():
        return {"location": "åŒ—äº¬", "temperature": "72", "unit": "fahrenheit"}
    elif "æˆéƒ½" in weather.location.lower():
        return {"location": "æˆéƒ½", "temperature": "22", "unit": "celsius"}
    else:
        return {"location": weather.location, "temperature": "ä¸çŸ¥é“"}

def get_stock_price(params: dict):
    stock_param = StockParams(**params)
    if 'AAPL' in stock_param.symbol:
        return {"symbol": "AAPL","name": "è‹¹æœ", "price": "202.82", "unit": "USD"}
    elif "AMZN" in stock_param.symbol:
        return {"symbol": "AMZN", "name": "äºšé©¬é€Š","price": "207.23", "unit": "USD"}
    else:
        return {"symbol": stock_param.symbol, "name": "","price": "ä¸çŸ¥é“", "unit": "USD"}
    
# å·¥å…·åˆ—è¡¨
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šä½ç½®çš„å½“å‰å¤©æ°”",
            "parameters": WeatherParams.model_json_schema()
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_stock_price",
            "description": "è·å–è‚¡ç¥¨æœ€æ–°ä»·æ ¼",
            "parameters": StockParams.model_json_schema()
        }
    }
]

tools
```




    [{'type': 'function',
      'function': {'name': 'get_weather',
       'description': 'è·å–æŒ‡å®šä½ç½®çš„å½“å‰å¤©æ°”',
       'parameters': {'properties': {'location': {'description': "åŸå¸‚åç§°ï¼Œå¦‚ 'åŒ—äº¬'",
          'title': 'Location',
          'type': 'string'},
         'unit': {'default': 'celsius',
          'description': 'æ¸©åº¦å•ä½',
          'title': 'Unit',
          'type': 'string'}},
        'required': ['location'],
        'title': 'WeatherParams',
        'type': 'object'}}},
     {'type': 'function',
      'function': {'name': 'get_stock_price',
       'description': 'è·å–è‚¡ç¥¨æœ€æ–°ä»·æ ¼',
       'parameters': {'properties': {'symbol': {'description': "è‚¡ç¥¨ä»£ç ï¼Œå¦‚ 'AAPL'",
          'title': 'Symbol',
          'type': 'string'},
         'timeframe': {'default': '1d',
          'description': 'æ—¶é—´èŒƒå›´',
          'title': 'Timeframe',
          'type': 'string'}},
        'required': ['symbol'],
        'title': 'StockParams',
        'type': 'object'}}}]



##### 2. å‘èµ·å‡½æ•°è°ƒç”¨è¯·æ±‚


```python
import asyncio, os
from typing import Union, List, Dict, Optional
import logging

logger = logging.getLogger(__name__)

model_config_dict = {
    "deepseek-chat": {
        "model": "openai/deepseek-chat",
        "base_url": "https://api.deepseek.com/v1",
        "api_key": os.getenv("DEEPSEEK_API_KEY"),
    },
}

def llm_chat(model: str, messages: Union[str, List[Dict[str, str]]], tools: Optional[List] = None, stream=True):
    logger.info(f'ä½¿ç”¨çš„æ¨¡å‹å¦‚ä¸‹ï¼š{model}')
    model_config = model_config_dict.get(model)
    if isinstance(messages, str):
        messages = [{"role": "user", "content": messages}]

    response = litellm.completion(
        model=model_config.get('model'),
        base_url=model_config.get('base_url'),
        api_key=model_config.get('api_key'),
        messages=messages,
        tools=tools,
        stream=stream,
    )
    
    return response

messages=[{"role": "user", "content": "ä¸Šæµ·ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿè‹¹æœè‚¡ä»·æ˜¯å¤šå°‘ï¼Ÿ"}]

response = llm_chat(
    model='deepseek-chat', 
    messages=messages, 
    tools=tools,
    stream=False
)

response
```




    ModelResponse(id='5de8bff5-fa6b-414e-9948-62f43d4abb07', created=1749126781, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{"location": "ä¸Šæµ·", "unit": "celsius"}', name='get_weather'), id='call_0_afd2cb42-f93d-45f6-9a40-32020366337b', type='function'), ChatCompletionMessageToolCall(index=1, function=Function(arguments='{"symbol": "AAPL"}', name='get_stock_price'), id='call_1_f800d8fb-3084-425f-bb5f-fc62a45bc047', type='function')], function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=48, prompt_tokens=323, total_tokens=371, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=320, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=320, prompt_cache_miss_tokens=3), service_tier=None)



##### 3. è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
åœ¨å‡½æ•°è°ƒç”¨çš„æµç¨‹ä¸­ï¼Œæ­£ç¡®çš„æ¶ˆæ¯é¡ºåºåº”è¯¥æ˜¯ï¼š

1. ç”¨æˆ·æ¶ˆæ¯ï¼ˆuserï¼‰æˆ–ç³»ç»Ÿæ¶ˆæ¯ï¼ˆsystemï¼‰

2. æ¨¡å‹çš„æ¶ˆæ¯ï¼Œå…¶ä¸­åŒ…å«äº† `tool_calls`ï¼ˆè¡¨ç¤ºæ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·ï¼‰

3. å·¥å…·æ¶ˆæ¯ï¼ˆtoolï¼‰ï¼Œä½œä¸ºå¯¹ `tool_calls` çš„å“åº”



```python
import json

available_functions = {
                "get_weather": get_weather,
                "get_stock_price": get_stock_price,
            }

def execute_tool_call(response, messages):
    """å¤„ç†å·¥å…·è°ƒç”¨å¹¶è¿”å›æ›´æ–°åçš„æ¶ˆæ¯åˆ—è¡¨"""
    if not hasattr(response.choices[0].message, 'tool_calls') or not response.choices[0].message.tool_calls:
        return messages, None
    tool_calls = response.choices[0].message.tool_calls
    updated_messages = messages.copy()
    
    # æ·»åŠ æ¨¡å‹çš„æ¶ˆæ¯ï¼ˆåŒ…å«tool_callsï¼‰
    # updated_messages.append({
    #     "role": "assistant",
    #     "content": None,
    #     "tool_calls": [
    #         {
    #             "id": call.id,
    #             "type": "function",
    #             "function": {
    #                 "name": call.function.name,
    #                 "arguments": call.function.arguments
    #             }
    #         } for call in tool_calls
    #     ]
    # })
    # æ·»åŠ æ¨¡å‹çš„æ¶ˆæ¯ï¼Œä¸å‰é¢æ„é€ çš„æ¨¡å‹æ¶ˆæ¯ç›¸ä¼¼
    updated_messages.append(response.choices[0].message)
    
    # æ‰§è¡Œå·¥å…·å¹¶æ·»åŠ ç»“æœ
    for tool_call in tool_calls:
        
        function_name = tool_call.function.name
        args = json.loads(tool_call.function.arguments)
        
        # æ‰§è¡Œå¯¹åº”å‡½æ•°
        function_to_call = available_functions[function_name]
        function_response = function_to_call(args)
       

        updated_messages.append({
            "role": "tool",
            "name": function_name,
            "content": json.dumps(function_response, ensure_ascii=False),
            "tool_call_id": tool_call.id
        })
    
    return updated_messages, tool_calls

updated_messages, tool_calls = execute_tool_call(response=response, messages=messages)
updated_messages, tool_calls

```




    ([{'role': 'user', 'content': 'ä¸Šæµ·ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿè‹¹æœè‚¡ä»·æ˜¯å¤šå°‘ï¼Ÿ'},
      Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{"location": "ä¸Šæµ·", "unit": "celsius"}', name='get_weather'), id='call_0_afd2cb42-f93d-45f6-9a40-32020366337b', type='function'), ChatCompletionMessageToolCall(index=1, function=Function(arguments='{"symbol": "AAPL"}', name='get_stock_price'), id='call_1_f800d8fb-3084-425f-bb5f-fc62a45bc047', type='function')], function_call=None, provider_specific_fields={'refusal': None}),
      {'role': 'tool',
       'name': 'get_weather',
       'content': '{"location": "ä¸Šæµ·", "temperature": "10", "unit": "celsius"}',
       'tool_call_id': 'call_0_afd2cb42-f93d-45f6-9a40-32020366337b'},
      {'role': 'tool',
       'name': 'get_stock_price',
       'content': '{"symbol": "AAPL", "name": "è‹¹æœ", "price": "202.82", "unit": "USD"}',
       'tool_call_id': 'call_1_f800d8fb-3084-425f-bb5f-fc62a45bc047'}],
     [ChatCompletionMessageToolCall(index=0, function=Function(arguments='{"location": "ä¸Šæµ·", "unit": "celsius"}', name='get_weather'), id='call_0_afd2cb42-f93d-45f6-9a40-32020366337b', type='function'),
      ChatCompletionMessageToolCall(index=1, function=Function(arguments='{"symbol": "AAPL"}', name='get_stock_price'), id='call_1_f800d8fb-3084-425f-bb5f-fc62a45bc047', type='function')])



##### 4. è·å–æœ€ç»ˆå“åº”
å°†å‡½æ•°æ‰§è¡Œç»“æœä¼ å›æ¨¡å‹ï¼ˆå¯é€‰ï¼Œç”¨äºå¤šè½®å¯¹è¯ï¼‰


```python
final_response = llm_chat(
    model='deepseek-chat', 
    messages=updated_messages, 
    stream=False
)

print("æœ€ç»ˆå›ç­”:", final_response.choices[0].message.content)

```

    /Users/xiniao/myenv/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:
      PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'index': 0, 'function': ...7b', 'type': 'function'}, input_type=dict])
      PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'index': 1, 'function': ...47', 'type': 'function'}, input_type=dict])
      return self.__pydantic_serializer__.to_python(


    æœ€ç»ˆå›ç­”: ä¸Šæµ·ç°åœ¨çš„å¤©æ°”æ˜¯10Â°Cã€‚è‹¹æœ(AAPL)çš„è‚¡ä»·ç›®å‰æ˜¯202.82ç¾å…ƒã€‚


## LiteLLM æ™ºèƒ½è·¯ç”±åŠŸèƒ½è¯¦è§£

LiteLLM çš„åŠ¨æ€è·¯ç”±æ˜¯å…¶æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ï¼Œå®ƒå…è®¸å¼€å‘è€…æ„å»ºæ™ºèƒ½çš„æ¨¡å‹è°ƒç”¨ç½‘å…³ï¼Œæ ¹æ®æˆæœ¬ã€æ€§èƒ½ã€å¯ç”¨æ€§ç­‰æŒ‡æ ‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹ã€‚

åœ¨ LiteLLM ä¸­æä¾›äº†`Router`ç±»ï¼Œ`Router`ç±»çš„ä½œç”¨è¦†ç›–äº†æ¨¡å‹ç®¡ç†ã€ç¼“å­˜ã€è°ƒåº¦ã€å¯é æ€§ã€å›é€€ç­–ç•¥ã€è·¯ç”±ç­–ç•¥ã€æ•…éšœå¤„ç†ã€é¢„ç®—æ§åˆ¶ç­‰å¤šä¸ªæ–¹é¢ï¼Œæä¾›äº†é«˜åº¦å¯é…ç½®çš„è·¯ç”±æœºåˆ¶ã€‚é€šè¿‡åˆç†é…ç½®è¿™äº›å‚æ•°ï¼Œå¯ä»¥æ„å»ºå‡ºé€‚åº”ä¸åŒåœºæ™¯éœ€æ±‚çš„æ™ºèƒ½æ¨¡å‹è·¯ç”±ç³»ç»Ÿã€‚

## LiteLLM Router å‚æ•°è¯¦è§£
æŸ¥çœ‹ `Router`ç±»çš„æºç å‚æ•°å¦‚ä¸‹ï¼š
```python
def __init__(  # noqa: PLR0915
        self,
        model_list: Optional[
            Union[List[DeploymentTypedDict], List[Dict[str, Any]]]
        ] = None,
        ## ASSISTANTS API ##
        assistants_config: Optional[AssistantsTypedDict] = None,
        ## CACHING ##
        redis_url: Optional[str] = None,
        redis_host: Optional[str] = None,
        redis_port: Optional[int] = None,
        redis_password: Optional[str] = None,
        cache_responses: Optional[bool] = False,
        cache_kwargs: dict = {},  # additional kwargs to pass to RedisCache (see caching.py)
        caching_groups: Optional[
            List[tuple]
        ] = None,  # if you want to cache across model groups
        client_ttl: int = 3600,  # ttl for cached clients - will re-initialize after this time in seconds
        ## SCHEDULER ##
        polling_interval: Optional[float] = None,
        default_priority: Optional[int] = None,
        ## RELIABILITY ##
        num_retries: Optional[int] = None,
        max_fallbacks: Optional[
            int
        ] = None,  # max fallbacks to try before exiting the call. Defaults to 5.
        timeout: Optional[float] = None,
        stream_timeout: Optional[float] = None,
        default_litellm_params: Optional[
            dict
        ] = None,  # default params for Router.chat.completion.create
        default_max_parallel_requests: Optional[int] = None,
        set_verbose: bool = False,
        debug_level: Literal["DEBUG", "INFO"] = "INFO",
        default_fallbacks: Optional[
            List[str]
        ] = None,  # generic fallbacks, works across all deployments
        fallbacks: List = [],
        context_window_fallbacks: List = [],
        content_policy_fallbacks: List = [],
        model_group_alias: Optional[
            Dict[str, Union[str, RouterModelGroupAliasItem]]
        ] = {},
        enable_pre_call_checks: bool = False,
        enable_tag_filtering: bool = False,
        retry_after: int = 0,  # min time to wait before retrying a failed request
        retry_policy: Optional[
            Union[RetryPolicy, dict]
        ] = None,  # set custom retries for different exceptions
        model_group_retry_policy: Dict[
            str, RetryPolicy
        ] = {},  # set custom retry policies based on model group
        allowed_fails: Optional[
            int
        ] = None,  # Number of times a deployment can failbefore being added to cooldown
        allowed_fails_policy: Optional[
            AllowedFailsPolicy
        ] = None,  # set custom allowed fails policy
        cooldown_time: Optional[
            float
        ] = None,  # (seconds) time to cooldown a deployment after failure
        disable_cooldowns: Optional[bool] = None,
        routing_strategy: Literal[
            "simple-shuffle",
            "least-busy",
            "usage-based-routing",
            "latency-based-routing",
            "cost-based-routing",
            "usage-based-routing-v2",
        ] = "simple-shuffle",
        optional_pre_call_checks: Optional[OptionalPreCallChecks] = None,
        routing_strategy_args: dict = {},  # just for latency-based
        provider_budget_config: Optional[GenericBudgetConfigType] = None,
        alerting_config: Optional[AlertingConfig] = None,
        router_general_settings: Optional[
            RouterGeneralSettings
        ] = RouterGeneralSettings(),
        ignore_invalid_deployments: bool = False,
    ) -> None:
```

ä¸ºäº†æ›´å¥½çš„ä½¿ç”¨`Router`ç±»ï¼Œä¸‹é¢æ˜¯å¯¹ `Router` ç±»åˆå§‹åŒ–å‚æ•°çš„è¯¦ç»†è§£é‡Šï¼ŒæŒ‰åŠŸèƒ½åˆ†ç±»è¯´æ˜ï¼š

### ä¸€ã€æ ¸å¿ƒæ¨¡å‹é…ç½®

 1. `model_list`
- **ä½œç”¨**ï¼šå®šä¹‰è·¯ç”±ç®¡ç†çš„æ¨¡å‹åˆ—è¡¨ï¼Œæ¯ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¨¡å‹åˆ«åå’Œå…·ä½“çš„è°ƒç”¨å‚æ•°ã€‚
- **æ ¼å¼**ï¼šå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ¨¡å‹é…ç½®
- **ç¤ºä¾‹**ï¼š
  ```python
  [
      {
          "model_name": "azure-gpt-4",  # æ¨¡å‹åˆ«å
          "litellm_params": {ã€‚# å…·ä½“çš„è°ƒç”¨å‚æ•°
              "model": "azure/gpt-4-turbo",  # å®é™…æ¨¡å‹å
              "api_key": "...",
              "api_base": "..."
          },
          "model_info": {"id": "gpt-4-turbo"}  # é¢å¤–æ¨¡å‹ä¿¡æ¯
      },
      # æ›´å¤šæ¨¡å‹...
  ]
  ```

2. `assistants_config`
- **ä½œç”¨**ï¼šé…ç½® OpenAI Assistants API ç›¸å…³å‚æ•°
- **ç±»å‹**ï¼šå­—å…¸
- **ç”¨é€”**ï¼šæ„å»ºåŸºäºåŠ©æ‰‹çš„å·¥ä½œæµ

### äºŒã€ç¼“å­˜é…ç½®

| å‚æ•° | ä½œç”¨ | é»˜è®¤å€¼ |
|------|------|--------|
| `redis_url` |RedisæœåŠ¡å™¨çš„URLï¼Œç”¨äºç¼“å­˜å“åº”ã€‚å¦‚æœæä¾›ï¼Œå°†è¦†ç›–`redis_host`å’Œ`redis_port` | `None` |
| `redis_host` | Redis ä¸»æœºåœ°å€ | `None` |
| `redis_port` | Redis ç«¯å£ | `None` |
| `redis_password` | Redis å¯†ç  | `None` |
| `cache_responses` | æ˜¯å¦ç¼“å­˜å“åº” | `False` |
| `cache_kwargs` | ä¼ é€’ç»™ç¼“å­˜ç³»ç»Ÿçš„é¢å¤–å‚æ•° | `{}` |
| `caching_groups` | è·¨æ¨¡å‹ç»„çš„ç¼“å­˜é…ç½® | `None` |
| `client_ttl` | å®¢æˆ·ç«¯ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰ | `3600` (1å°æ—¶) |

### ä¸‰ã€è°ƒåº¦ä¸é˜Ÿåˆ—

| å‚æ•° | ä½œç”¨ | å¤‡æ³¨ |
|------|------|------|
| `polling_interval` | è½®è¯¢é˜Ÿåˆ—çš„é¢‘ç‡ï¼ˆç§’ï¼‰ | ç”¨äº `.scheduler_acompletion()` |
| `default_priority` | è¯·æ±‚çš„é»˜è®¤ä¼˜å…ˆçº§,ä¼˜å…ˆçº§é«˜çš„è¯·æ±‚ä¼šè¢«ä¼˜å…ˆå¤„ç† | `None` |

### å››ã€å¯é æ€§é…ç½®

| å‚æ•° | ä½œç”¨ | é»˜è®¤å€¼ |
|------|------|--------|
| `num_retries` | å¤±è´¥è¯·æ±‚çš„é‡è¯•æ¬¡æ•° | `None` |
| `max_fallbacks` | æœ€å¤§å›é€€å°è¯•æ¬¡æ•° | `5` |
| `timeout` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | `None` |
| `stream_timeout` | æµå¼è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | `None` |
| `retry_after` | é‡è¯•å‰æœ€å°ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ | `0` |
| `retry_policy` | ä½¿ç”¨`RetryPolicy`è‡ªå®šä¹‰é‡è¯•ç­–ç•¥ | `None` |
| `model_group_retry_policy` | æŒ‰æ¨¡å‹ç»„çš„é‡è¯•ç­–ç•¥ | `{}` |

### äº”ã€å›é€€ç­–ç•¥

| å‚æ•° | ä½œç”¨ | ç¤ºä¾‹ |
|------|------|------|
| `default_fallbacks` | é€šç”¨å›é€€åˆ—è¡¨ï¼Œé€‚ç”¨äºæ‰€æœ‰éƒ¨ç½²ã€‚å½“ä¸»æ¨¡å‹å¤±è´¥æ—¶ï¼Œä¼šå°è¯•å›é€€åˆ°åˆ—è¡¨ä¸­çš„æ¨¡å‹ã€‚ | `["gpt-3.5-turbo"]` |
| `fallbacks` | ç‰¹å®šæ¨¡å‹çš„å›é€€é“¾ | `[{"gpt-4": "claude-3"}]` |
| `context_window_fallbacks` | ä¸Šä¸‹æ–‡çª—å£ä¸è¶³æ—¶çš„å›é€€ | `[{"llama3": "claude-3"}]` |
| `content_policy_fallbacks` | å†…å®¹ç­–ç•¥è¿è§„æ—¶çš„å›é€€ | `[{"gpt-4": "gemini"}]` |

### å…­ã€è·¯ç”±ç­–ç•¥

1. `routing_strategy`
- **ä½œç”¨**ï¼šé€‰æ‹©è·¯ç”±ç®—æ³•
- **å¯é€‰å€¼**ï¼š
    - `"simple-shuffle"`ï¼šç®€å•éšæœºé€‰æ‹©ï¼ˆé»˜è®¤ï¼‰
    - `"least-busy"`ï¼šé€‰æ‹©å½“å‰è¯·æ±‚æœ€å°‘çš„æ¨¡å‹
    - `"usage-based-routing"`ï¼šåŸºäºä½¿ç”¨é‡ï¼ˆå¦‚Tokenæ¶ˆè€—ï¼‰çš„è·¯ç”±
    - `"latency-based-routing"`ï¼šåŸºäºå»¶è¿Ÿçš„è·¯ç”±
    - `"cost-based-routing"`ï¼šåŸºäºæˆæœ¬çš„è·¯ç”±
    - `"usage-based-routing-v2"`ï¼šæ”¹è¿›ç‰ˆçš„ä½¿ç”¨é‡è·¯ç”±
    
- **é»˜è®¤å€¼**ï¼š`"simple-shuffle"`

2. `routing_strategy_args`
- **ä½œç”¨**ï¼šè·¯ç”±ç­–ç•¥çš„é¢å¤–å‚æ•°
- **ç±»å‹**ï¼šå­—å…¸
- **ç”¨é€”**ï¼šä¼ é€’ç­–ç•¥ç‰¹å®šå‚æ•°ï¼Œå¦‚æˆæœ¬æƒé‡

### ä¸ƒã€æ¨¡å‹å¥åº·ä¸æ•…éšœå¤„ç†

| å‚æ•° | ä½œç”¨ | é»˜è®¤å€¼ |
|------|------|--------|
| `allowed_fails` | æ•…éšœé˜ˆå€¼ï¼ˆè§¦å‘å†·å´çš„å¤±è´¥æ¬¡æ•°ï¼‰ | `None` |
| `allowed_fails_policy` | ä½¿ç”¨`AllowedFailsPolicy`è‡ªå®šä¹‰æ•…éšœç­–ç•¥ | `None` |
| `cooldown_time` | æ¨¡å‹å†·å´æ—¶é—´ï¼ˆç§’ï¼‰ | `1` |
| `disable_cooldowns` | æ˜¯å¦ç¦ç”¨å†·å´æœºåˆ¶ | `None` |

### å…«ã€é¢„æ£€æŸ¥ä¸è¿‡æ»¤

| å‚æ•° | ä½œç”¨ | é»˜è®¤å€¼ |
|------|------|--------|
| `enable_pre_call_checks` | æ˜¯å¦åœ¨è°ƒç”¨å‰æ£€æŸ¥éƒ¨ç½²çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ã€‚å¦‚æœæç¤ºè¶…å‡ºé™åˆ¶ï¼Œåˆ™è¿‡æ»¤æ‰è¯¥éƒ¨ç½² | `False` |
| `enable_tag_filtering` | å¯ç”¨æ ‡ç­¾è¿‡æ»¤ | `False` |
| `optional_pre_call_checks` | å¯é€‰çš„é¢„è°ƒç”¨æ£€æŸ¥é…ç½® | `None` |

### ä¹ã€æ¨¡å‹åˆ«åä¸åˆ†ç»„

| å‚æ•° | ä½œç”¨ | ç¤ºä¾‹ |
|------|------|------|
| `model_group_alias` | æ¨¡å‹ç»„åˆ«åé…ç½® | `{"fast-group": ["gpt-4", "claude-3"]}` |

### åã€é¢„ç®—ä¸å‘Šè­¦

| å‚æ•° | ä½œç”¨ | ç±»å‹ |
|------|------|------|
| `provider_budget_config` | è®¾ç½®LLMæä¾›å•†çš„é¢„ç®—é™åˆ¶ã€‚ä¾‹å¦‚ï¼Œæ¯å¤©ä¸ºOpenAIè®¾ç½®100ç¾å…ƒé¢„ç®—ã€‚ | å­—å…¸ |
| `alerting_config` | å‘Šè­¦é…ç½®ï¼ˆå¦‚Slacké›†æˆï¼‰å½“å‘ç”Ÿç‰¹å®šäº‹ä»¶ï¼ˆå¦‚éƒ¨ç½²å¤±è´¥ï¼‰æ—¶å‘é€å‘Šè­¦ | `AlertingConfig` |

### åä¸€ã€è°ƒè¯•ä¸æ—¥å¿—

| å‚æ•° | ä½œç”¨ | é»˜è®¤å€¼ |
|------|------|--------|
| `set_verbose` | å¯ç”¨è¯¦ç»†æ—¥å¿— | `False` |
| `debug_level` | æ—¥å¿—çº§åˆ«ï¼ˆ"DEBUG"/"INFO"ï¼‰ | `"INFO"` |
| `ignore_invalid_deployments` | æ˜¯å¦å¿½ç•¥æ— æ•ˆçš„éƒ¨ç½²ã€‚å¦‚æœä¸ºTrueï¼Œåˆ™åˆå§‹åŒ–æ—¶é‡åˆ°æ— æ•ˆéƒ¨ç½²ä¸ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œæ˜¯è·³è¿‡å¹¶ç»§ç»­ã€‚é»˜è®¤ä¸ºFalseã€‚ | `False` |

### åäºŒã€é«˜çº§é…ç½®

| å‚æ•° | ä½œç”¨ | è¯´æ˜ |
|------|------|------|
| `default_litellm_params` | é»˜è®¤è°ƒç”¨å‚æ•° | å…¨å±€å‚æ•°è¦†ç›– |
| `default_max_parallel_requests` | é»˜è®¤æœ€å¤§å¹¶è¡Œè¯·æ±‚æ•° | æ§åˆ¶å¹¶å‘ |
| `router_general_settings` | è·¯ç”±å™¨é€šç”¨è®¾ç½® | `RouterGeneralSettings` å¯¹è±¡ |

## å‚æ•°åŠŸèƒ½å…³ç³»å›¾
```mermaid
graph TD
    A[æ¨¡å‹åˆ—è¡¨ model_list] --> B[è·¯ç”±ç­–ç•¥ routing_strategy]
    B --> C{è·¯ç”±å†³ç­–}
    C --> D[ç¼“å­˜é…ç½® redis_urlç­‰]
    C --> E[å›é€€ç­–ç•¥ fallbacks]
    C --> F[æ•…éšœå¤„ç† allowed_fails]
    D --> G[å“åº”è¿”å›]
    E --> G
    F --> G
    H[ç›‘æ§å‘Šè­¦ alerting_config] --> F
    I[é¢„æ£€æŸ¥ enable_pre_call_checks] --> C
```

## å…¸å‹é…ç½®ç¤ºä¾‹

```python
from litellm import Router

router = Router(
    model_list=[
        {
            "model_name": "gpt-4-turbo",
            "litellm_params": {
                "model": "azure/gpt-4-turbo",
                "api_key": "azure-key",
                "api_base": "https://azure-endpoint"
            },
            "model_info": {"cost": 0.00003}
        },
        {
            "model_name": "claude-3-haiku",
            "litellm_params": {
                "model": "anthropic/claude-3-haiku",
                "api_key": "anthropic-key"
            },
            "model_info": {"cost": 0.00001}
        }
    ],
    routing_strategy="cost-based-routing",
    redis_url="redis://localhost:6379",
    cache_responses=True,
    num_retries=3,
    timeout=30,
    fallbacks=[{"gpt-4-turbo": "claude-3-haiku"}],
    allowed_fails=3,
    cooldown_time=60,
    set_verbose=True
)
```

## å…³é”®åŠŸèƒ½è¯´æ˜

1. **è·¯ç”±ç­–ç•¥é€‰æ‹©æŒ‡å—**ï¼š
   ```mermaid
    graph TD
        A[é€‰æ‹©è·¯ç”±ç­–ç•¥] --> B{å…³é”®éœ€æ±‚}
        B -->|æˆæœ¬ä¼˜åŒ–| C[cost-based-routing]
        B -->|æœ€ä½å»¶è¿Ÿ| D[latency-based-routing]
        B -->|é«˜å¹¶å‘| E[least-busy]
        B -->|é¢„ç®—æ§åˆ¶| F[usage-based-routing]
        B -->|é«˜å¯ç”¨| G[model-fallback]
        B -->|ç®€å•å‡è¡¡| H[simple-shuffle]
        B -->|è‡ªå®šä¹‰é€»è¾‘| I[custom-function]
   ```

2. **æ•…éšœå¤„ç†æµç¨‹**ï¼š
   - æ¨¡å‹å¤±è´¥æ¬¡æ•°è¶…è¿‡ `allowed_fails` é˜ˆå€¼
   - è¿›å…¥ `cooldown_time` ç§’çš„å†·å´æœŸ
   - è·¯ç”±è‡ªåŠ¨åˆ‡æ¢åˆ°å›é€€æ¨¡å‹
   - å†·å´ç»“æŸåé‡æ–°å°è¯•

3. **ç¼“å­˜æœºåˆ¶**ï¼š
   - ä½¿ç”¨ Redis å­˜å‚¨å“åº”
   - `client_ttl` æ§åˆ¶ç¼“å­˜æœ‰æ•ˆæœŸ
   - ç›¸åŒè¯·æ±‚ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ

4. **é¢„ç®—æ§åˆ¶**ï¼š
   - é€šè¿‡ `provider_budget_config` è®¾ç½®é¢„ç®—
   - è¶…è¿‡é¢„ç®—è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶ä»–æä¾›å•†
   - æ”¯æŒæŒ‰å¤©/å‘¨/æœˆè®¾ç½®é™é¢

## æ ¸å¿ƒä»·å€¼

1. **æ™ºèƒ½è´Ÿè½½å‡è¡¡**ï¼šåœ¨å¤šä¸ªæ¨¡å‹/ä¾›åº”å•†é—´åˆ†é…è¯·æ±‚
2. **æˆæœ¬ä¼˜åŒ–**ï¼šè‡ªåŠ¨é€‰æ‹©æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡å‹
3. **é«˜å¯ç”¨æ€§**ï¼šæ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹
4. **æµé‡æ§åˆ¶**ï¼šæŒ‰é…é¢åˆ†é…è°ƒç”¨é‡
5. **A/Bæµ‹è¯•**ï¼šæ— ç¼å¯¹æ¯”æ¨¡å‹æ€§èƒ½


## è·¯ç”±ç­–ç•¥å®æˆ˜é…ç½®

### éšæœºè·¯ç”±é…ç½®




```python

from litellm import Router

# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "my-llm-test",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
        },
    },
    {
        "model_name": "my-llm-test",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": os.getenv("BAILIAN_API_KEY"),
        },
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="simple-shuffle",  # é»˜è®¤è·¯ç”±ç­–ç•¥ simple-shuffleï¼Œéšæœºé€‰æ‹©æ¨¡å‹
    set_verbose=True
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

# ä½¿ç”¨è·¯ç”±
response = router.completion(
    model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
    messages=messages
)

print(response)

# ä½¿ç”¨è·¯ç”±
response = router.completion(
    model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
    messages=messages
)

print(response)

```

    11:15:13 - LiteLLM Router:INFO: router.py:660 - Routing strategy: simple-shuffle
    11:15:14 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/qwen-turbo)[32m 200 OK


    ModelResponse(id='chatcmpl-6c983ebb-54f1-9b7b-8385-347de1515a04', created=1749179715, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œè¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=64, prompt_tokens=10, total_tokens=74, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)


    11:15:21 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/deepseek-chat)[32m 200 OK


    ModelResponse(id='36c23345-aa7f-4ed8-a525-19d7eb869a3e', created=1749179715, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰åˆ›é€ çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼âœ¨ æˆ‘çš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œè¿˜èƒ½å¸®ä½ å¤„ç†å„ç§æ–‡æœ¬ã€æ–‡æ¡£ç­‰å†…å®¹ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å°ç–‘é—®ï¼Œéƒ½å¯ä»¥æ¥æ‰¾æˆ‘å“¦ï¼ğŸ˜Š  \n\næœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=70, prompt_tokens=4, total_tokens=74, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None)


## åŸºäºæƒé‡çš„éšæœºè·¯ç”±
å¯¹æ¨¡å‹è®¾ç½®æƒé‡`litellm_params["weight"]`ä»¥æ¯”å…¶ä»–æ¨¡å‹æ›´é¢‘ç¹åœ°é€‰æ‹©ä¸€ç§è·¯ç”±ï¼Œè¿™é€‚ç”¨äº`simple-shuffle`è·¯ç”±ç­–ç•¥ï¼ˆå¦‚æœæœªé€‰æ‹©è·¯ç”±ç­–ç•¥ï¼Œè¿™æ˜¯é»˜è®¤å€¼ï¼‰ã€‚



```python

from litellm import Router

# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "my-llm-test",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
            "weight": 1,
        },
    },
    {
        "model_name": "my-llm-test",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": os.getenv("BAILIAN_API_KEY"),
            "weight": 2,  # é€‰æ‹©æ­¤æ¨¡å‹æ¯”å‰ä¸€ä¸ªæ›´é¢‘ç¹2å€
        },
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="simple-shuffle",  # é»˜è®¤è·¯ç”±ç­–ç•¥ simple-shuffleï¼Œéšæœºé€‰æ‹©æ¨¡å‹
    set_verbose=True
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

# ä½¿ç”¨è·¯ç”±
response = router.completion(
    model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
    messages=messages
)

print(response)

# ä½¿ç”¨è·¯ç”±
response = router.completion(
    model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
    messages=messages
)

print(response)

# ä½¿ç”¨è·¯ç”±
response = router.completion(
    model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
    messages=messages
)

print(response)

```

    11:35:29 - LiteLLM Router:INFO: router.py:660 - Routing strategy: simple-shuffle
    11:35:29 - LiteLLM Router:INFO: simple_shuffle.py:55 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo', 'weight': 2}, 'model_info': {'id': '7c6b876813711f84e643255a92267a1ecd18a012c6df37bbfa010ca6572a54eb', 'db_model': False, 'cost': 1e-05}} for model: my-llm-test
    11:35:30 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/qwen-turbo)[32m 200 OK
    11:35:30 - LiteLLM Router:INFO: simple_shuffle.py:55 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://api.deepseek.com/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/deepseek-chat', 'weight': 1}, 'model_info': {'id': '7cc6bbf33429d66def887cba459d00b997960200174b741ccee0d683662febb6', 'db_model': False, 'cost': 3e-05}} for model: my-llm-test


    ModelResponse(id='chatcmpl-595a9128-72f4-9406-b6ea-cafd8b9741dc', created=1749180931, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ã€ç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=52, prompt_tokens=10, total_tokens=62, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)


    11:35:37 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/deepseek-chat)[32m 200 OK
    11:35:37 - LiteLLM Router:INFO: simple_shuffle.py:55 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo', 'weight': 2}, 'model_info': {'id': '7c6b876813711f84e643255a92267a1ecd18a012c6df37bbfa010ca6572a54eb', 'db_model': False, 'cost': 1e-05}} for model: my-llm-test


    ModelResponse(id='47f1ccde-679d-4b97-ba8d-3c2ffc797c57', created=1749180931, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸æ‰“é€ çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼âœ¨ æˆ‘çš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®ã€é™ªä½ èŠå¤©ï¼Œè¿˜èƒ½å¤„ç†å„ç§æ–‡æœ¬ã€æ–‡æ¡£ä¿¡æ¯ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å°å›°æƒ‘ï¼Œéƒ½å¯ä»¥æ¥æ‰¾æˆ‘èŠèŠï¼ğŸ˜Š  \n\næœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=64, prompt_tokens=4, total_tokens=68, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None)


    11:35:38 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/qwen-turbo)[32m 200 OK


    ModelResponse(id='chatcmpl-31cded70-dda4-98ff-bf1c-acd593954f76', created=1749180939, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ä»»åŠ¡ï¼Œå¹¶æ”¯æŒå¤šç§è¯­è¨€ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=63, prompt_tokens=10, total_tokens=73, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)


## åŸºäºæœ€ä½å»¶è¿Ÿè·¯ç”±é…ç½®

é€‰æ‹©å“åº”æ—¶é—´æœ€çŸ­çš„æ¨¡å‹ï¼Œå®ƒæ ¹æ®ä»æ¨¡å‹å‘é€å’Œæ¥æ”¶è¯·æ±‚çš„æ—¶é—´ç¼“å­˜å¹¶æ›´æ–°æ¨¡å‹çš„å“åº”æ—¶é—´ã€‚



```python

from litellm import Router

# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "my-llm-test",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
        },
        "model_info": {"id":"deepseek-chat"} 
    },
    {
        "model_name": "my-llm-test",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": os.getenv("BAILIAN_API_KEY"),
        },
        "model_info": {"id":"qwen-turbo"}
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="latency-based-routing",  # é€‰æ‹©åŸºäºæœ€ä½å»¶è¿Ÿè·¯ç”±é…ç½®
    set_verbose=True
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]
tasks = []
# å¾ªç¯è°ƒç”¨ï¼Œç¼“å­˜å¹¶æ›´æ–°æ¨¡å‹çš„å“åº”æ—¶é—´ï¼Œåœ¨åç»­çš„è°ƒç”¨ä¸­é€‰æ‹©å“åº”æ—¶é—´æœ€çŸ­çš„æ¨¡å‹
for _ in range(5):
    tasks.append(router.acompletion(model="my-llm-test", messages=messages))
response = await asyncio.gather(*tasks)
print(response)


# ä½¿ç”¨è·¯ç”±
response = router.completion(
    model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
    messages=messages
)

print(response)



```

    12:00:54 - LiteLLM Router:INFO: router.py:660 - Routing strategy: latency-based-routing
    12:00:54 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://api.deepseek.com/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/deepseek-chat'}, 'model_info': {'id': 'deepseek-chat', 'db_model': False, 'cost': 3e-05}} for model: my-llm-test
    12:00:54 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False, 'cost': 1e-05}} for model: my-llm-test
    12:00:54 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False, 'cost': 1e-05}} for model: my-llm-test
    12:00:54 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False, 'cost': 1e-05}} for model: my-llm-test
    12:00:54 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://api.deepseek.com/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/deepseek-chat'}, 'model_info': {'id': 'deepseek-chat', 'db_model': False, 'cost': 3e-05}} for model: my-llm-test
    12:00:56 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/qwen-turbo)[32m 200 OK
    12:00:56 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/qwen-turbo)[32m 200 OK
    12:00:56 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/qwen-turbo)[32m 200 OK
    12:01:00 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/deepseek-chat)[32m 200 OK
    12:01:01 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/deepseek-chat)[32m 200 OK
    12:01:01 - LiteLLM Router:INFO: router.py:6448 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False, 'cost': 1e-05}} for model: my-llm-test


    [ModelResponse(id='c62a00d2-410c-4e8e-ad93-c2692c01231e', created=1749182454, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰ç ”å‘çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œæ¯”å¦‚å­¦ä¹ ã€å·¥ä½œã€ç§‘æŠ€ã€ç”Ÿæ´»å°çªé—¨ï¼Œç”šè‡³é™ªä½ èŠå¤©è§£é—·ã€‚æœ‰ä»€ä¹ˆæƒ³é—®çš„ï¼Œå°½ç®¡æ¥å§ï¼ğŸ˜Š', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=59, prompt_tokens=4, total_tokens=63, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None), ModelResponse(id='chatcmpl-3b50779a-a89d-9edf-b94d-ac02558db233', created=1749182456, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œè¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=64, prompt_tokens=10, total_tokens=74, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None), ModelResponse(id='chatcmpl-0a80858c-b588-9efc-a633-de9042621458', created=1749182457, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=57, prompt_tokens=10, total_tokens=67, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None), ModelResponse(id='chatcmpl-6996d035-f7f3-90f2-baa0-5b05bf83fb41', created=1749182457, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=56, prompt_tokens=10, total_tokens=66, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None), ModelResponse(id='9d3fe083-a7d4-437c-a83d-4a5936570846', created=1749182454, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰ç ”å‘çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼ğŸ¤–âœ¨  \n\næˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œã€ç¼–ç¨‹ï¼Œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å°å›°æƒ‘ï¼Œéƒ½å¯ä»¥æ¥é—®æˆ‘ï¼å¦‚æœä½ æœ‰ä»€ä¹ˆéœ€è¦ï¼Œå°½ç®¡å¼€å£å§~ ğŸ˜Š', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=59, prompt_tokens=4, total_tokens=63, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None)]


    12:01:02 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/qwen-turbo)[32m 200 OK


    ModelResponse(id='chatcmpl-1f8f351f-2858-9e7c-9de1-33249ac4b589', created=1749182463, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ã€ç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=53, prompt_tokens=10, total_tokens=63, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)


## åŸºäºæœ€å°‘è¯·æ±‚è·¯ç”±é…ç½®
é€‰æ‹©å½“å‰è¯·æ±‚æœ€å°‘çš„æ¨¡å‹


```python

from litellm import Router

# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "my-llm-test",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
        },
        "model_info": {"id":"deepseek-chat"}
    },
    {
        "model_name": "my-llm-test",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": os.getenv("BAILIAN_API_KEY"),
        },
        "model_info": {"id":"qwen-turbo"}
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="least-busy",  # é€‰æ‹©åŸºäºæœ€å°‘è¯·æ±‚è·¯ç”±é…ç½®
    set_verbose=True
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]
tasks = []
# å¾ªç¯è°ƒç”¨
for _ in range(2):
    tasks.append(router.acompletion(model="my-llm-test", messages=messages))
response = await asyncio.gather(*tasks)
print(response)


# ä½¿ç”¨è·¯ç”±
response = router.completion(
    model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
    messages=messages
)

print(response)



```

    13:35:20 - LiteLLM Router:INFO: router.py:660 - Routing strategy: least-busy
    13:35:20 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://api.deepseek.com/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/deepseek-chat'}, 'model_info': {'id': 'deepseek-chat', 'db_model': False, 'cost': 3e-05}} for model: my-llm-test
    13:35:20 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://api.deepseek.com/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/deepseek-chat'}, 'model_info': {'id': 'deepseek-chat', 'db_model': False, 'cost': 3e-05}} for model: my-llm-test
    13:35:26 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/deepseek-chat)[32m 200 OK
    13:35:31 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/deepseek-chat)[32m 200 OK
    13:35:31 - LiteLLM Router:INFO: router.py:6448 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False, 'cost': 1e-05}} for model: my-llm-test


    [ModelResponse(id='0249d7e2-ceef-426d-9d43-62d43008be46', created=1749188120, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰åˆ›é€ çš„æ™ºèƒ½å¯¹è¯åŠ©æ‰‹ï¼ğŸ˜Š æˆ‘çš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„å°å›°æƒ‘ï¼Œæˆ‘éƒ½ä¼šå°½åŠ›æä¾›æœ‰ç”¨çš„ä¿¡æ¯å’Œå»ºè®®ã€‚  \n\nä½ å¯ä»¥é—®æˆ‘ä»»ä½•é—®é¢˜ï¼Œæ¯”å¦‚ï¼š  \n- **çŸ¥è¯†ç§‘æ™®**ï¼šé»‘æ´æ˜¯æ€ä¹ˆå½¢æˆçš„ï¼Ÿ  \n- **å­¦ä¹ è¾…å¯¼**ï¼šå¦‚ä½•æé«˜è‹±è¯­å†™ä½œèƒ½åŠ›ï¼Ÿ  \n- **å®ç”¨å»ºè®®**ï¼šæœ‰ä»€ä¹ˆæ—¶é—´ç®¡ç†çš„å¥½æ–¹æ³•ï¼Ÿ  \n- **å¨±ä¹ä¼‘é—²**ï¼šæ¨èå‡ æœ¬å¥½çœ‹çš„å°è¯´å§ï¼  \n\næˆ‘ç›®å‰æ˜¯**å…è´¹**çš„ï¼Œè€Œä¸”æ”¯æŒ**é•¿æ–‡æœ¬å¯¹è¯ï¼ˆ128Kä¸Šä¸‹æ–‡ï¼‰**ï¼Œè¿˜èƒ½è¯»å–ä½ ä¸Šä¼ çš„**å›¾ç‰‡ã€PDFã€Wordã€Excelç­‰æ–‡ä»¶**ï¼Œå¹¶ä»ä¸­æå–æ–‡å­—ä¿¡æ¯æ¥å¸®åŠ©ä½ åˆ†æå†…å®¹ã€‚  \n\næœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜ƒ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=166, prompt_tokens=4, total_tokens=170, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None), ModelResponse(id='65ea6058-2214-42a1-9a51-d3178cd83539', created=1749188120, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ã€æä¾›çŸ¥è¯†æ”¯æŒã€é™ªä½ èŠå¤©ï¼Œç”šè‡³å¸®ä½ å¤„ç†æ–‡æ¡£å’Œæ–‡ä»¶ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=48, prompt_tokens=4, total_tokens=52, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None)]


    13:35:32 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/qwen-turbo)[32m 200 OK


    ModelResponse(id='chatcmpl-ca30bd33-b748-9ff2-9cf9-dcc254c88712', created=1749188132, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=52, prompt_tokens=10, total_tokens=62, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)


## åŸºäºæœ€ä½æˆæœ¬è·¯ç”±é…ç½®
æœ€ä½æˆæœ¬è·¯ç”±ç­–ç•¥çš„å·¥ä½œåŸç†ã€‚

1. è·å–æ‰€æœ‰å¥åº·çš„éƒ¨ç½²ï¼ˆdeploymentï¼‰æ¨¡å‹

2. é€‰æ‹©é‚£äº›åœ¨å…¶æä¾›çš„rpmï¼ˆæ¯åˆ†é’Ÿè¯·æ±‚æ•°ï¼‰å’Œtpmï¼ˆæ¯åˆ†é’Ÿtokenæ•°ï¼‰é™åˆ¶å†…çš„éƒ¨ç½²

3. å¯¹äºæ¯ä¸ªéƒ¨ç½²ï¼Œæ£€æŸ¥å®ƒæ˜¯å¦åœ¨[litellm_model_cost_map](https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json)ï¼ˆæˆæœ¬æ˜ å°„è¡¨ï¼‰ä¸­å­˜åœ¨

- å¦‚æœéƒ¨ç½²ä¸å­˜åœ¨äºæˆæœ¬æ˜ å°„è¡¨ä¸­ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æˆæœ¬ï¼ˆ1ç¾å…ƒï¼‰

4. é€‰æ‹©æˆæœ¬æœ€ä½çš„éƒ¨ç½²

å¯ä»¥è®¾ç½®`litellm_params["input_cost_per_token"]`å’Œ`litellm_params["output_cost_per_token"]`ä»¥ä¾¿åœ¨è·¯ç”±æ—¶ä½¿ç”¨è‡ªå®šä¹‰æˆæœ¬ã€‚
å¦‚æœè®¾ç½®äº†è‡ªå®šä¹‰æˆæœ¬ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„æˆæœ¬è¿›è¡Œæ¯”è¾ƒã€‚



```python

from litellm import Router
import asyncio
# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "my-llm-test",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
            "input_cost_per_token": 0.00003,  # æ¯Tokenæˆæœ¬
            "output_cost_per_token": 0.00003,
        },
        "model_info": {"id":"deepseek-chat"}  
    },
    {
        "model_name": "my-llm-test",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": os.getenv("BAILIAN_API_KEY"),
            "input_cost_per_token": 0.00001,
            "output_cost_per_token": 0.00001,
        },
        "model_info": {"id":"qwen-turbo"}
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="cost-based-routing",  # é€‰æ‹©åŸºäºæœ€ä½æˆæœ¬è·¯ç”±é…ç½®
    set_verbose=True
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]
tasks = []
# å¾ªç¯è°ƒç”¨
for _ in range(2):
    tasks.append(router.acompletion(model="my-llm-test", messages=messages))
response = await asyncio.gather(*tasks)
print(response)


```

    13:57:07 - LiteLLM Router:INFO: router.py:660 - Routing strategy: cost-based-routing
    13:57:07 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    13:57:07 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    13:57:07 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    13:57:07 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    13:57:07 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'input_cost_per_token': 1e-05, 'output_cost_per_token': 1e-05, 'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False, 'input_cost_per_token': 1e-05, 'output_cost_per_token': 1e-05}} for model: my-llm-test
    13:57:07 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'input_cost_per_token': 1e-05, 'output_cost_per_token': 1e-05, 'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False, 'input_cost_per_token': 1e-05, 'output_cost_per_token': 1e-05}} for model: my-llm-test
    13:57:08 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/qwen-turbo)[32m 200 OK
    13:57:09 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/qwen-turbo)[32m 200 OK


    [ModelResponse(id='chatcmpl-73379046-922b-9e10-9193-cda015839781', created=1749189429, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ä»»åŠ¡ï¼Œå¹¶æ”¯æŒå¤šè¯­è¨€äº¤æµã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=65, prompt_tokens=10, total_tokens=75, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None), ModelResponse(id='chatcmpl-b7cfda24-8c92-9e7d-b840-0ec80285c323', created=1749189429, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=50, prompt_tokens=10, total_tokens=60, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)]


    /var/folders/ln/2_qqbq993pv1yc5c8_dm5q6h0000gp/T/ipykernel_1818/4150574972.py:41: RuntimeWarning: coroutine 'Router.acompletion' was never awaited
      response = await asyncio.gather(*tasks)
    RuntimeWarning: Enable tracemalloc to get the object allocation traceback


## åŸºäºæœ€å°‘ä½¿ç”¨è·¯ç”±é…ç½®

è¿™å°†è·¯ç”±åˆ°tpm(æ¯åˆ†é’Ÿtokenæ•°)ä½¿ç”¨ç‡æœ€ä½çš„éƒ¨ç½²ã€‚

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨Redisæ¥è·Ÿè¸ªå¤šä¸ªéƒ¨ç½²ä¸­tpm(æ¯åˆ†é’Ÿtokenæ•°)çš„ä½¿ç”¨æƒ…å†µã€‚

å¦‚æœä¼ å…¥æ¨¡å‹çš„tpm/rpmé™åˆ¶ï¼Œè¿™ä¹Ÿå°†å¯¹æ­¤è¿›è¡Œæ£€æŸ¥ï¼Œå¹¶è¿‡æ»¤æ‰ä»»ä½•è¶…å‡ºé™åˆ¶çš„æ¨¡å‹ã€‚

å¯¹äºAzureï¼Œrpm(æ¯åˆ†é’Ÿè¯·æ±‚æ•°)=tpm(æ¯åˆ†é’Ÿtokenæ•°)/6ã€‚



```python

from litellm import Router
import asyncio
# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "my-llm-test",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
            "tpm": 1000,  # æ¯åˆ†é’Ÿtokenæ•°
            "rpm": 100,  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°
        },
        "model_info": {"id":"deepseek-chat"}  
    },
    {
        "model_name": "my-llm-test",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": os.getenv("BAILIAN_API_KEY"),
            "tpm": 100000,
            "rpm": 10000,
        },
        "model_info": {"id":"qwen-turbo"}
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="usage-based-routing-v2",  # é€‰æ‹©åŸºäºæœ€å°‘ä½¿ç”¨æƒ…å†µè·¯ç”±é…ç½®
    set_verbose=True,
    # cache_responses=True  # åœ¨ç”Ÿäº§ä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨Redisç¼“å­˜ã€‚ä¸ºäº†åœ¨æœ¬åœ°å¿«é€Ÿæµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒç®€å•çš„å†…å­˜ç¼“å­˜ã€‚
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]
tasks = []
# å¾ªç¯è°ƒç”¨
for _ in range(2):
    tasks.append(router.acompletion(model="my-llm-test", messages=messages))
response = await asyncio.gather(*tasks)
print(response)


```

    14:20:05 - LiteLLM Router:INFO: router.py:660 - Routing strategy: usage-based-routing-v2
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'tpm': 100000, 'rpm': 10000, 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/qwen-turbo'}, 'model_info': {'id': 'qwen-turbo', 'db_model': False}} for model: my-llm-test
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM Router:INFO: router.py:6292 - get_available_deployment for model: my-llm-test, Selected deployment: {'model_name': 'my-llm-test', 'litellm_params': {'api_key': 'sk**********', 'api_base': 'https://api.deepseek.com/v1', 'tpm': 1000, 'rpm': 100, 'use_in_pass_through': False, 'use_litellm_proxy': False, 'merge_reasoning_content_in_choices': False, 'model': 'openai/deepseek-chat'}, 'model_info': {'id': 'deepseek-chat', 'db_model': False}} for model: my-llm-test
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:05 - LiteLLM:WARNING: logging_callback_manager.py:124 - Cannot add callback - would exceed MAX_CALLBACKS limit of 30. Current callbacks: 30
    14:20:10 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/qwen-turbo)[32m 200 OK
    14:20:12 - LiteLLM Router:INFO: router.py:1103 - litellm.acompletion(model=openai/deepseek-chat)[32m 200 OK


    [ModelResponse(id='chatcmpl-c1b6361b-864f-9a9f-8f4f-121204e778ff', created=1749190810, model='qwen-turbo', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ã€ç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=53, prompt_tokens=10, total_tokens=63, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None), ModelResponse(id='59af6dbe-193f-4e30-b4de-c8d01a988e48', created=1749190806, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸ï¼ˆDeepSeekï¼‰ç ”å‘çš„AIåŠ©æ‰‹ï¼ğŸ¤–âœ¨ æˆ‘çš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œç”šè‡³å¸®ä½ å¤„ç†å„ç§æ–‡æœ¬å’Œæ–‡ä»¶å†…å®¹ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç–‘é—®ï¼Œéƒ½å¯ä»¥æ¥æ‰¾æˆ‘ï¼ğŸ˜Š  \n\næœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=70, prompt_tokens=4, total_tokens=74, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None)]


## å¤±è´¥é‡è¯•

- å¯¹äºasyncå’Œsyncå‡½æ•°éƒ½æ”¯æŒé‡è¯•å¤±è´¥çš„è¯·æ±‚ã€‚

- å¯¹äºRateLimitErrorï¼Œå®ç°äº†æŒ‡æ•°é€€é¿ã€‚

- å¯¹äºä¸€èˆ¬é”™è¯¯ï¼Œä¼šç«‹å³é‡è¯•

- æ”¯æŒåœ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚ä¹‹å‰è®¾ç½®æœ€çŸ­ç­‰å¾…æ—¶é—´ï¼Œå¯ä»¥é€šè¿‡retry_afterå‚æ•°è®¾ç½®ã€‚

ä¸‹é¢å¿«é€Ÿæµè§ˆä¸€ä¸‹æˆ‘ä»¬å¦‚ä½•è®¾ç½®num_retries=3ï¼š


```python
import os
from litellm import Router
from openai import OpenAIError
import asyncio
# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "my-llm-test",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
        },
        "model_info": {"id":"deepseek-chat"}  
    },
    {
        "model_name": "my-llm-test",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            # "api_key": os.getenv("BAILIAN_API_KEY"),
        },
        "model_info": {"id":"qwen-turbo"}
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="simple-shuffle",  
    set_verbose=True,
    num_retries=3,  # å¤±è´¥é‡è¯• 3 æ¬¡
    retry_after=5,  # é‡è¯•å¤±è´¥çš„è¯·æ±‚ä¹‹å‰ç­‰å¾… 5 ç§’
    # cache_responses=True  # åœ¨ç”Ÿäº§ä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨Redisç¼“å­˜ã€‚ä¸ºäº†åœ¨æœ¬åœ°å¿«é€Ÿæµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒç®€å•çš„å†…å­˜ç¼“å­˜ã€‚
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

try:
    # ä½¿ç”¨è·¯ç”±
    response = router.completion(
        model="my-llm-test",  # ä½¿ç”¨æ¨¡å‹åˆ«åè°ƒç”¨
        messages=messages
    )
    
    print(response)
except OpenAIError as e:
    print(str(e))


```

    14:36:00 - LiteLLM Router:INFO: router.py:660 - Routing strategy: simple-shuffle
    14:36:00 - LiteLLM Router:INFO: router.py:911 - litellm.completion(model=openai/deepseek-chat)[31m Exception litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
    14:36:00 - LiteLLM Router:INFO: router.py:3670 - Retrying request with num_retries: 3
    14:36:00 - LiteLLM Router:INFO: router.py:911 - litellm.completion(model=openai/qwen-turbo)[31m Exception litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
    14:36:05 - LiteLLM Router:INFO: router.py:911 - litellm.completion(model=openai/qwen-turbo)[31m Exception litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
    14:36:05 - LiteLLM Router:INFO: router.py:911 - litellm.completion(model=openai/deepseek-chat)[31m Exception litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
    14:36:10 - LiteLLM Router:INFO: router.py:3379 - Trying to fallback b/w models


    litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable. Received Model Group=my-llm-test
    Available Model Group Fallbacks=None LiteLLM Retried: 2 times, LiteLLM Max Retries: 3


## æ¨¡å‹è°ƒç”¨å¤±è´¥å›é€€ç­–ç•¥é…ç½®
Fallbacksï¼ˆå›é€€æœºåˆ¶ï¼‰æ˜¯ LiteLLM çš„æ ¸å¿ƒé«˜å¯ç”¨ç‰¹æ€§ï¼Œå®ƒèƒ½åœ¨ä¸»æ¨¡å‹è°ƒç”¨å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹ï¼Œç¡®ä¿æœåŠ¡çš„è¿ç»­æ€§å’Œç¨³å®šæ€§ã€‚ä»¥ä¸‹æ˜¯å…¨é¢çš„åŠŸèƒ½è§£æï¼š

æ ¹æ®ä¹‹å‰æä¾›çš„ Router å‚æ•°ï¼Œæˆ‘ä»¬çŸ¥é“æœ‰å‡ ç§ç±»å‹çš„å›é€€é…ç½®ï¼š

1. `default_fallbacks`: é€šç”¨å›é€€åˆ—è¡¨ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ï¼Œè§¦å‘æ¡ä»¶ï¼šä»»ä½•æ¨¡å‹è°ƒç”¨å¤±è´¥æ—¶

2. `fallbacks`: æŒ‡å®šç‰¹å®šæ¨¡å‹ç»„çš„å›é€€é“¾ï¼Œç²¾ç»†åŒ–æ§åˆ¶å›é€€è·¯å¾„

3. `context_window_fallbacks`: å½“è¾“å…¥è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£æ—¶çš„å›é€€ï¼Œè§¦å‘æ¡ä»¶ï¼šæ”¶åˆ°é”™è¯¯`ContextWindowExceededError`ï¼Œå…¸å‹åœºæ™¯ï¼šå¤„ç†é•¿æ–‡æ¡£æ‘˜è¦ã€ä»£ç åˆ†æ

4. `content_policy_fallbacks`: å½“å†…å®¹è¿åæ¨¡å‹ä½¿ç”¨ç­–ç•¥æ—¶çš„å›é€€ï¼Œè§¦å‘æ¡ä»¶ï¼š æ”¶åˆ°é”™è¯¯`PolicyViolationError`ï¼Œåº”ç”¨åœºæ™¯ï¼šåŒ»ç–—/é‡‘èç­‰æ•æ„Ÿé¢†åŸŸ

æ­¤å¤–ï¼Œåœ¨æ•…éšœå¤„ç†æ–¹é¢ï¼Œè¿˜æœ‰ `max_fallbacks` å‚æ•°æ§åˆ¶æœ€å¤§å›é€€å°è¯•æ¬¡æ•°ã€‚

**å›é€€é¡ºåºï¼šä¸»æ¨¡å‹å¤±è´¥ â†’ å®šå‘å›é€€ï¼ˆfallbacksï¼‰ â†’ é€šç”¨å›é€€ï¼ˆdefault_fallbacksï¼‰ â†’ æœ€ç»ˆå…œåº•**

**å›é€€ç­–ç•¥æ‰§è¡Œé¡ºåºæµç¨‹å›¾**
```mermaid
graph TD
    A[ä¸»æ¨¡å‹è°ƒç”¨] --> B{å¤±è´¥åŸå› ?}
    B -->|å¸¸è§„é”™è¯¯| C[è§¦å‘fallbackså®šå‘å›é€€]
    B -->|ä¸Šä¸‹æ–‡è¶…é™ï¼ŒæŠ›å‡ºå¼‚å¸¸ContextWindowExceededError| D[è§¦å‘context_window_fallbacks]
    B -->|ç­–ç•¥è¿è§„ï¼ŒæŠ›å‡ºå¼‚å¸¸PolicyViolationError| E[è§¦å‘content_policy_fallbacks]
    C --> F{æ˜¯å¦ç”¨å°½fallbacks?}
    D --> G{æ˜¯å¦ç”¨å°½contextå›é€€?}
    E --> H{æ˜¯å¦ç”¨å°½policyå›é€€?}
    F -->|æ˜¯| I[è§¦å‘default_fallbacks]
    G -->|æ˜¯| I
    H -->|æ˜¯| I
    I --> J{æ˜¯å¦ç”¨å°½defaultå›é€€?}
    J -->|æ˜¯| K[æŠ›å‡ºæœ€ç»ˆé”™è¯¯]
```

**Fallbacks å·¥ä½œæµç¨‹**
```mermaid
sequenceDiagram
    participant Client
    participant Router
    participant ModelA
    participant ModelB
    participant ModelC
    
    Client->>Router: è¯·æ±‚(gpt-4-turbo)
    Router->>ModelA: è°ƒç”¨ä¸»æ¨¡å‹
    ModelA-->>Router: å¤±è´¥(è¶…æ—¶)
    Router->>ModelB: å›é€€(claude-3-opus)
    ModelB-->>Router: å¤±è´¥(ç­–ç•¥è¿è§„)
    Router->>ModelC: å›é€€(deepseek-chat)
    ModelC-->>Router: æˆåŠŸå“åº”
    Router-->>Client: è¿”å›ç»“æœ
```
### å®Œæ•´å·¥ä½œæµç¤ºä¾‹



```python
import os
from litellm import Router
from openai import OpenAIError
import asyncio
# å®šä¹‰æ¨¡å‹æ± 
model_list = [
    {
        "model_name": "deepseek-chat",  # æ¨¡å‹åˆ«å->å…·æœ‰ç›¸åŒ'model_name'çš„æ¨¡å‹ä¹‹é—´çš„è´Ÿè½½å¹³è¡¡
        "litellm_params": {  # litellm completion/embedding å®é™…è°ƒç”¨å‚æ•°
            "model": "openai/deepseek-chat",
            "api_base": "https://api.deepseek.com/v1",
            "api_key": os.getenv("DEEPSEEK_API_KEY"),
        },
        "model_info": {"id":"deepseek-chat"}  
    },
    {
        "model_name": "qwen-turbo",
        "litellm_params": {
            "model": "openai/qwen-turbo",
            "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            # "api_key": os.getenv("BAILIAN_API_KEY"),
        },
        "model_info": {"id":"qwen-turbo"}
    },
]

# åˆ›å»ºè·¯ç”±
router = Router(
    model_list=model_list,
    routing_strategy="simple-shuffle",  
    set_verbose=True,

    # å›é€€ç­–ç•¥
    default_fallbacks=["deepseek-chat"],
    fallbacks=[{"qwen-turbo": "claude-3-opus"}],
    context_window_fallbacks=[{"qwen-turbo": "deepseek-chat"}],
    content_policy_fallbacks=[{"qwen-turbo": "deepseek-chat"}],
    
    # é‡è¯•é…ç½®
    num_retries=3,  # å¤±è´¥é‡è¯• 3 æ¬¡
    retry_after=5,  # é‡è¯•å¤±è´¥çš„è¯·æ±‚ä¹‹å‰ç­‰å¾… 5 ç§’
    # cache_responses=True  # åœ¨ç”Ÿäº§ä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨Redisç¼“å­˜ã€‚ä¸ºäº†åœ¨æœ¬åœ°å¿«é€Ÿæµ‹è¯•ï¼Œæˆ‘ä»¬è¿˜æ”¯æŒç®€å•çš„å†…å­˜ç¼“å­˜ã€‚
)

messages=[{"role": "user", "content": "ä½ æ˜¯è°"}]

try:
    # ä½¿ç”¨è·¯ç”±
    response = router.completion(
        model="qwen-turbo", 
        messages=messages
    )
    
    print(response)
except OpenAIError as e:
    print(str(e))


```

    15:20:23 - LiteLLM Router:INFO: router.py:660 - Routing strategy: simple-shuffle
    15:20:23 - LiteLLM Router:INFO: router.py:911 - litellm.completion(model=openai/qwen-turbo)[31m Exception litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
    15:20:23 - LiteLLM Router:INFO: router.py:3379 - Trying to fallback b/w models
    15:20:23 - LiteLLM Router:INFO: fallback_event_handlers.py:128 - Falling back to model_group = c
    15:20:23 - LiteLLM Router:INFO: router.py:911 - litellm.completion(model=None)[31m Exception litellm.BadRequestError: You passed in model=c. There is no 'model_name' with this string 
    15:20:23 - LiteLLM Router:INFO: router.py:3379 - Trying to fallback b/w models
    15:20:23 - LiteLLM Router:INFO: fallback_event_handlers.py:128 - Falling back to model_group = deepseek-chat
    15:20:31 - LiteLLM Router:INFO: router.py:893 - litellm.completion(model=openai/deepseek-chat)[32m 200 OK
    15:20:31 - LiteLLM Router:INFO: fallback_event_handlers.py:142 - Successful fallback b/w models.
    15:20:31 - LiteLLM Router:INFO: fallback_event_handlers.py:142 - Successful fallback b/w models.


    ModelResponse(id='f2e51863-6047-49fe-a562-db168ded247e', created=1749194424, model='deepseek-chat', object='chat.completion', system_fingerprint='fp_8802369eaa_prod0425fp8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸å¼€å‘çš„æ™ºèƒ½AIåŠ©æ‰‹ï¼âœ¨ æˆ‘çš„ä½¿å‘½æ˜¯å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€é™ªä½ èŠå¤©ï¼Œè¿˜èƒ½å¸®ä½ å¤„ç†å„ç§æ–‡æœ¬ã€æ–‡ä»¶å†…å®¹ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œï¼Œè¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å°å›°æƒ‘ï¼Œéƒ½å¯ä»¥æ¥æ‰¾æˆ‘èŠèŠï¼ğŸ˜Š  \n\næœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}))], usage=Usage(completion_tokens=65, prompt_tokens=4, total_tokens=69, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=4), service_tier=None)


## å°† jupyter notebook è½¬æ¢ä¸º markdown æ ¼å¼

å®‰è£…nbconverter: `pip install nbconvert`

æ‰§è¡Œå‘½ä»¤ï¼š`jupyter nbconvert --to FORMAT notebook.ipynb`

è¿™é‡ŒFORMAT ç”¨å…·ä½“çš„æ ¼å¼æ›¿æ¢ï¼Œå¦‚ markdown, htmlç­‰


```python
!jupyter nbconvert --to markdown litellm_use.ipynb
```

    [NbConvertApp] Converting notebook litellm_use.ipynb to markdown
    [NbConvertApp] Writing 103765 bytes to litellm_use.md

